{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c7084085",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accepted</th>\n",
       "      <th>actually</th>\n",
       "      <th>afar</th>\n",
       "      <th>africa</th>\n",
       "      <th>aging</th>\n",
       "      <th>ago</th>\n",
       "      <th>air</th>\n",
       "      <th>alcoholic</th>\n",
       "      <th>align</th>\n",
       "      <th>allah</th>\n",
       "      <th>...</th>\n",
       "      <th>world</th>\n",
       "      <th>worried</th>\n",
       "      <th>worse</th>\n",
       "      <th>worst</th>\n",
       "      <th>write</th>\n",
       "      <th>writer</th>\n",
       "      <th>written</th>\n",
       "      <th>wrote</th>\n",
       "      <th>yes</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175 rows × 572 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     accepted  actually  afar  africa  aging  ago  air  alcoholic  align  \\\n",
       "0           0         0     0       0      0    0    0          0      0   \n",
       "1           0         0     0       0      0    0    0          0      0   \n",
       "2           0         0     0       0      0    0    0          0      0   \n",
       "3           0         0     0       0      0    0    0          0      0   \n",
       "4           0         0     0       0      0    0    0          0      0   \n",
       "..        ...       ...   ...     ...    ...  ...  ...        ...    ...   \n",
       "170         0         0     0       0      0    0    0          0      0   \n",
       "171         0         0     0       0      0    0    0          0      0   \n",
       "172         0         0     0       0      0    0    0          0      0   \n",
       "173         0         0     0       0      0    0    0          0      0   \n",
       "174         0         0     0       0      0    0    0          0      0   \n",
       "\n",
       "     allah  ...  world  worried  worse  worst  write  writer  written  wrote  \\\n",
       "0        0  ...      0        0      0      0      0       1        0      0   \n",
       "1        0  ...      0        0      0      0      0       0        0      0   \n",
       "2        0  ...      0        0      0      0      0       0        0      0   \n",
       "3        0  ...      0        0      0      0      0       0        0      0   \n",
       "4        0  ...      0        0      0      0      0       0        0      0   \n",
       "..     ...  ...    ...      ...    ...    ...    ...     ...      ...    ...   \n",
       "170      0  ...      0        0      0      0      0       0        0      0   \n",
       "171      0  ...      0        0      0      0      0       0        0      0   \n",
       "172      0  ...      0        0      0      0      0       0        0      0   \n",
       "173      0  ...      0        0      0      0      0       0        0      0   \n",
       "174      0  ...      0        0      0      0      0       0        0      0   \n",
       "\n",
       "     yes  young  \n",
       "0      0      0  \n",
       "1      0      0  \n",
       "2      0      0  \n",
       "3      0      0  \n",
       "4      0      0  \n",
       "..   ...    ...  \n",
       "170    0      0  \n",
       "171    0      0  \n",
       "172    0      0  \n",
       "173    0      0  \n",
       "174    0      0  \n",
       "\n",
       "[175 rows x 572 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ef8f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4d76bb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accepted</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actually</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>afar</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>africa</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aging</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1    2    3    4    5    6    7    8    9    ...  165  166  \\\n",
       "accepted    0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "actually    0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "afar        0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "africa      0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "aging       0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "\n",
       "          167  168  169  170  171  172  173  174  \n",
       "accepted    0    0    0    0    0    0    0    0  \n",
       "actually    0    0    0    0    0    0    0    0  \n",
       "afar        0    0    0    0    0    0    0    0  \n",
       "africa      0    0    0    0    0    0    0    0  \n",
       "aging       0    0    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 175 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "036311f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6aeee5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "22350489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 17:19:05,829 : INFO : using symmetric alpha at 0.5\n",
      "2022-05-03 17:19:05,831 : INFO : using symmetric eta at 0.5\n",
      "2022-05-03 17:19:05,832 : INFO : using serial LDA version on this node\n",
      "2022-05-03 17:19:05,834 : INFO : running online (multi-pass) LDA training, 2 topics, 10 passes over the supplied corpus of 175 documents, updating model once every 175 documents, evaluating perplexity every 175 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-05-03 17:19:05,951 : INFO : -7.246 per-word bound, 151.8 perplexity estimate based on a held-out corpus of 175 documents with 902 words\n",
      "2022-05-03 17:19:05,951 : INFO : PROGRESS: pass 0, at document #175/175\n",
      "2022-05-03 17:19:06,029 : INFO : topic #0 (0.500): 0.014*\"ve\" + 0.007*\"somebody\" + 0.006*\"ancient\" + 0.006*\"god\" + 0.006*\"believe\" + 0.006*\"artist\" + 0.006*\"believed\" + 0.005*\"success\" + 0.005*\"chemical\" + 0.005*\"looking\"\n",
      "2022-05-03 17:19:06,029 : INFO : topic #1 (0.500): 0.008*\"allah\" + 0.007*\"love\" + 0.006*\"god\" + 0.006*\"idea\" + 0.006*\"dance\" + 0.006*\"century\" + 0.005*\"knew\" + 0.005*\"feel\" + 0.005*\"divine\" + 0.005*\"make\"\n",
      "2022-05-03 17:19:06,030 : INFO : topic diff=0.639943, rho=1.000000\n",
      "2022-05-03 17:19:06,078 : INFO : -6.833 per-word bound, 114.0 perplexity estimate based on a held-out corpus of 175 documents with 902 words\n",
      "2022-05-03 17:19:06,079 : INFO : PROGRESS: pass 1, at document #175/175\n",
      "2022-05-03 17:19:06,111 : INFO : topic #0 (0.500): 0.015*\"ve\" + 0.007*\"ancient\" + 0.007*\"somebody\" + 0.007*\"artist\" + 0.006*\"believe\" + 0.006*\"believed\" + 0.006*\"success\" + 0.006*\"chemical\" + 0.006*\"looking\" + 0.005*\"say\"\n",
      "2022-05-03 17:19:06,113 : INFO : topic #1 (0.500): 0.009*\"allah\" + 0.007*\"love\" + 0.007*\"god\" + 0.007*\"dance\" + 0.007*\"knew\" + 0.006*\"idea\" + 0.005*\"feel\" + 0.005*\"writer\" + 0.005*\"changed\" + 0.005*\"century\"\n",
      "2022-05-03 17:19:06,113 : INFO : topic diff=0.193075, rho=0.577350\n",
      "2022-05-03 17:19:06,155 : INFO : -6.775 per-word bound, 109.5 perplexity estimate based on a held-out corpus of 175 documents with 902 words\n",
      "2022-05-03 17:19:06,155 : INFO : PROGRESS: pass 2, at document #175/175\n",
      "2022-05-03 17:19:06,191 : INFO : topic #0 (0.500): 0.015*\"ve\" + 0.008*\"ancient\" + 0.007*\"artist\" + 0.007*\"somebody\" + 0.007*\"believe\" + 0.006*\"believed\" + 0.006*\"success\" + 0.006*\"chemical\" + 0.006*\"looking\" + 0.006*\"say\"\n",
      "2022-05-03 17:19:06,192 : INFO : topic #1 (0.500): 0.010*\"allah\" + 0.008*\"god\" + 0.007*\"dance\" + 0.007*\"love\" + 0.007*\"knew\" + 0.006*\"idea\" + 0.006*\"changed\" + 0.006*\"writer\" + 0.005*\"feel\" + 0.005*\"century\"\n",
      "2022-05-03 17:19:06,192 : INFO : topic diff=0.108594, rho=0.500000\n",
      "2022-05-03 17:19:06,228 : INFO : -6.751 per-word bound, 107.7 perplexity estimate based on a held-out corpus of 175 documents with 902 words\n",
      "2022-05-03 17:19:06,229 : INFO : PROGRESS: pass 3, at document #175/175\n",
      "2022-05-03 17:19:06,264 : INFO : topic #0 (0.500): 0.015*\"ve\" + 0.008*\"ancient\" + 0.008*\"artist\" + 0.007*\"somebody\" + 0.007*\"believe\" + 0.006*\"believed\" + 0.006*\"success\" + 0.006*\"chemical\" + 0.006*\"looking\" + 0.006*\"say\"\n",
      "2022-05-03 17:19:06,265 : INFO : topic #1 (0.500): 0.011*\"allah\" + 0.009*\"god\" + 0.008*\"dance\" + 0.007*\"love\" + 0.007*\"knew\" + 0.007*\"idea\" + 0.006*\"changed\" + 0.006*\"writer\" + 0.005*\"feel\" + 0.005*\"called\"\n",
      "2022-05-03 17:19:06,265 : INFO : topic diff=0.062419, rho=0.447214\n",
      "2022-05-03 17:19:06,300 : INFO : -6.741 per-word bound, 107.0 perplexity estimate based on a held-out corpus of 175 documents with 902 words\n",
      "2022-05-03 17:19:06,301 : INFO : PROGRESS: pass 4, at document #175/175\n",
      "2022-05-03 17:19:06,325 : INFO : topic #0 (0.500): 0.015*\"ve\" + 0.008*\"ancient\" + 0.008*\"artist\" + 0.007*\"somebody\" + 0.007*\"believe\" + 0.006*\"believed\" + 0.006*\"chemical\" + 0.006*\"success\" + 0.006*\"looking\" + 0.006*\"say\"\n",
      "2022-05-03 17:19:06,325 : INFO : topic #1 (0.500): 0.011*\"allah\" + 0.010*\"god\" + 0.008*\"dance\" + 0.008*\"love\" + 0.007*\"knew\" + 0.007*\"idea\" + 0.006*\"changed\" + 0.006*\"writer\" + 0.005*\"feel\" + 0.005*\"called\"\n",
      "2022-05-03 17:19:06,326 : INFO : topic diff=0.038485, rho=0.408248\n",
      "2022-05-03 17:19:06,361 : INFO : -6.736 per-word bound, 106.6 perplexity estimate based on a held-out corpus of 175 documents with 902 words\n",
      "2022-05-03 17:19:06,361 : INFO : PROGRESS: pass 5, at document #175/175\n",
      "2022-05-03 17:19:06,385 : INFO : topic #0 (0.500): 0.015*\"ve\" + 0.008*\"ancient\" + 0.008*\"artist\" + 0.007*\"somebody\" + 0.007*\"believe\" + 0.006*\"believed\" + 0.006*\"chemical\" + 0.006*\"success\" + 0.006*\"looking\" + 0.006*\"say\"\n",
      "2022-05-03 17:19:06,386 : INFO : topic #1 (0.500): 0.011*\"allah\" + 0.010*\"god\" + 0.008*\"dance\" + 0.008*\"love\" + 0.007*\"knew\" + 0.007*\"idea\" + 0.006*\"changed\" + 0.006*\"writer\" + 0.006*\"called\" + 0.005*\"feel\"\n",
      "2022-05-03 17:19:06,386 : INFO : topic diff=0.025486, rho=0.377964\n",
      "2022-05-03 17:19:06,423 : INFO : -6.732 per-word bound, 106.3 perplexity estimate based on a held-out corpus of 175 documents with 902 words\n",
      "2022-05-03 17:19:06,423 : INFO : PROGRESS: pass 6, at document #175/175\n",
      "2022-05-03 17:19:06,447 : INFO : topic #0 (0.500): 0.015*\"ve\" + 0.008*\"ancient\" + 0.008*\"artist\" + 0.007*\"somebody\" + 0.007*\"believe\" + 0.006*\"believed\" + 0.006*\"chemical\" + 0.006*\"success\" + 0.006*\"looking\" + 0.006*\"source\"\n",
      "2022-05-03 17:19:06,448 : INFO : topic #1 (0.500): 0.011*\"allah\" + 0.010*\"god\" + 0.009*\"dance\" + 0.008*\"love\" + 0.007*\"knew\" + 0.007*\"idea\" + 0.006*\"changed\" + 0.006*\"writer\" + 0.006*\"called\" + 0.006*\"applause\"\n",
      "2022-05-03 17:19:06,448 : INFO : topic diff=0.018179, rho=0.353553\n",
      "2022-05-03 17:19:06,486 : INFO : -6.730 per-word bound, 106.2 perplexity estimate based on a held-out corpus of 175 documents with 902 words\n",
      "2022-05-03 17:19:06,487 : INFO : PROGRESS: pass 7, at document #175/175\n",
      "2022-05-03 17:19:06,508 : INFO : topic #0 (0.500): 0.015*\"ve\" + 0.008*\"ancient\" + 0.008*\"artist\" + 0.007*\"somebody\" + 0.007*\"believe\" + 0.006*\"believed\" + 0.006*\"chemical\" + 0.006*\"success\" + 0.006*\"looking\" + 0.006*\"source\"\n",
      "2022-05-03 17:19:06,509 : INFO : topic #1 (0.500): 0.012*\"allah\" + 0.011*\"god\" + 0.009*\"dance\" + 0.008*\"love\" + 0.008*\"knew\" + 0.007*\"idea\" + 0.006*\"changed\" + 0.006*\"writer\" + 0.006*\"called\" + 0.006*\"applause\"\n",
      "2022-05-03 17:19:06,509 : INFO : topic diff=0.013991, rho=0.333333\n",
      "2022-05-03 17:19:06,540 : INFO : -6.728 per-word bound, 106.0 perplexity estimate based on a held-out corpus of 175 documents with 902 words\n",
      "2022-05-03 17:19:06,541 : INFO : PROGRESS: pass 8, at document #175/175\n",
      "2022-05-03 17:19:06,561 : INFO : topic #0 (0.500): 0.015*\"ve\" + 0.009*\"ancient\" + 0.008*\"artist\" + 0.007*\"somebody\" + 0.007*\"believe\" + 0.006*\"believed\" + 0.006*\"chemical\" + 0.006*\"looking\" + 0.006*\"success\" + 0.006*\"source\"\n",
      "2022-05-03 17:19:06,561 : INFO : topic #1 (0.500): 0.012*\"allah\" + 0.011*\"god\" + 0.009*\"dance\" + 0.008*\"love\" + 0.008*\"knew\" + 0.007*\"idea\" + 0.006*\"changed\" + 0.006*\"writer\" + 0.006*\"called\" + 0.006*\"applause\"\n",
      "2022-05-03 17:19:06,562 : INFO : topic diff=0.011160, rho=0.316228\n",
      "2022-05-03 17:19:06,592 : INFO : -6.726 per-word bound, 105.8 perplexity estimate based on a held-out corpus of 175 documents with 902 words\n",
      "2022-05-03 17:19:06,593 : INFO : PROGRESS: pass 9, at document #175/175\n",
      "2022-05-03 17:19:06,613 : INFO : topic #0 (0.500): 0.015*\"ve\" + 0.009*\"ancient\" + 0.009*\"artist\" + 0.007*\"somebody\" + 0.007*\"believe\" + 0.006*\"believed\" + 0.006*\"chemical\" + 0.006*\"looking\" + 0.006*\"success\" + 0.006*\"source\"\n",
      "2022-05-03 17:19:06,613 : INFO : topic #1 (0.500): 0.012*\"allah\" + 0.011*\"god\" + 0.009*\"dance\" + 0.008*\"love\" + 0.008*\"knew\" + 0.007*\"idea\" + 0.006*\"changed\" + 0.006*\"writer\" + 0.006*\"called\" + 0.006*\"question\"\n",
      "2022-05-03 17:19:06,614 : INFO : topic diff=0.009041, rho=0.301511\n",
      "2022-05-03 17:19:06,615 : INFO : topic #0 (0.500): 0.015*\"ve\" + 0.009*\"ancient\" + 0.009*\"artist\" + 0.007*\"somebody\" + 0.007*\"believe\" + 0.006*\"believed\" + 0.006*\"chemical\" + 0.006*\"looking\" + 0.006*\"success\" + 0.006*\"source\"\n",
      "2022-05-03 17:19:06,615 : INFO : topic #1 (0.500): 0.012*\"allah\" + 0.011*\"god\" + 0.009*\"dance\" + 0.008*\"love\" + 0.008*\"knew\" + 0.007*\"idea\" + 0.006*\"changed\" + 0.006*\"writer\" + 0.006*\"called\" + 0.006*\"question\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.015*\"ve\" + 0.009*\"ancient\" + 0.009*\"artist\" + 0.007*\"somebody\" + 0.007*\"believe\" + 0.006*\"believed\" + 0.006*\"chemical\" + 0.006*\"looking\" + 0.006*\"success\" + 0.006*\"source\"'),\n",
       " (1,\n",
       "  '0.012*\"allah\" + 0.011*\"god\" + 0.009*\"dance\" + 0.008*\"love\" + 0.008*\"knew\" + 0.007*\"idea\" + 0.006*\"changed\" + 0.006*\"writer\" + 0.006*\"called\" + 0.006*\"question\"')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "59fe173a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 17:19:06,627 : INFO : using symmetric alpha at 0.25\n",
      "2022-05-03 17:19:06,628 : INFO : using symmetric eta at 0.25\n",
      "2022-05-03 17:19:06,629 : INFO : using serial LDA version on this node\n",
      "2022-05-03 17:19:06,631 : INFO : running online (multi-pass) LDA training, 4 topics, 10 passes over the supplied corpus of 175 documents, updating model once every 175 documents, evaluating perplexity every 175 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-05-03 17:19:06,694 : INFO : -8.614 per-word bound, 391.9 perplexity estimate based on a held-out corpus of 175 documents with 902 words\n",
      "2022-05-03 17:19:06,695 : INFO : PROGRESS: pass 0, at document #175/175\n",
      "2022-05-03 17:19:06,750 : INFO : topic #0 (0.250): 0.008*\"somebody\" + 0.008*\"heard\" + 0.007*\"love\" + 0.007*\"feel\" + 0.007*\"brilliant\" + 0.007*\"distance\" + 0.007*\"thought\" + 0.007*\"great\" + 0.007*\"bother\" + 0.007*\"rational\"\n",
      "2022-05-03 17:19:06,751 : INFO : topic #1 (0.250): 0.014*\"ve\" + 0.011*\"knew\" + 0.011*\"happened\" + 0.009*\"world\" + 0.008*\"artist\" + 0.007*\"looking\" + 0.007*\"anybody\" + 0.007*\"manage\" + 0.007*\"tom\" + 0.007*\"trying\"\n",
      "2022-05-03 17:19:06,751 : INFO : topic #2 (0.250): 0.010*\"chemical\" + 0.010*\"question\" + 0.009*\"divine\" + 0.008*\"tom\" + 0.008*\"idea\" + 0.008*\"engineer\" + 0.008*\"reputation\" + 0.008*\"lit\" + 0.008*\"doomed\" + 0.007*\"example\"\n",
      "2022-05-03 17:19:06,752 : INFO : topic #3 (0.250): 0.014*\"god\" + 0.014*\"ve\" + 0.014*\"allah\" + 0.013*\"dance\" + 0.012*\"started\" + 0.010*\"ancient\" + 0.007*\"believe\" + 0.007*\"happen\" + 0.007*\"idea\" + 0.007*\"source\"\n",
      "2022-05-03 17:19:06,753 : INFO : topic diff=2.277080, rho=1.000000\n",
      "2022-05-03 17:19:06,786 : INFO : -7.049 per-word bound, 132.4 perplexity estimate based on a held-out corpus of 175 documents with 902 words\n",
      "2022-05-03 17:19:06,786 : INFO : PROGRESS: pass 1, at document #175/175\n",
      "2022-05-03 17:19:06,808 : INFO : topic #0 (0.250): 0.009*\"love\" + 0.008*\"allah\" + 0.008*\"somebody\" + 0.008*\"heard\" + 0.007*\"feel\" + 0.007*\"thought\" + 0.007*\"brilliant\" + 0.007*\"distance\" + 0.007*\"great\" + 0.007*\"bother\"\n",
      "2022-05-03 17:19:06,810 : INFO : topic #1 (0.250): 0.014*\"ve\" + 0.011*\"knew\" + 0.011*\"happened\" + 0.010*\"world\" + 0.009*\"job\" + 0.008*\"artist\" + 0.007*\"talking\" + 0.007*\"looking\" + 0.007*\"anybody\" + 0.007*\"success\"\n",
      "2022-05-03 17:19:06,811 : INFO : topic #2 (0.250): 0.010*\"chemical\" + 0.010*\"question\" + 0.010*\"divine\" + 0.008*\"idea\" + 0.008*\"tom\" + 0.008*\"engineer\" + 0.008*\"reputation\" + 0.008*\"lit\" + 0.008*\"doomed\" + 0.008*\"example\"\n",
      "2022-05-03 17:19:06,812 : INFO : topic #3 (0.250): 0.015*\"ve\" + 0.014*\"god\" + 0.013*\"allah\" + 0.013*\"dance\" + 0.012*\"started\" + 0.010*\"ancient\" + 0.008*\"applause\" + 0.007*\"believe\" + 0.007*\"source\" + 0.007*\"happen\"\n",
      "2022-05-03 17:19:06,812 : INFO : topic diff=0.101290, rho=0.577350\n",
      "2022-05-03 17:19:06,841 : INFO : -7.005 per-word bound, 128.5 perplexity estimate based on a held-out corpus of 175 documents with 902 words\n",
      "2022-05-03 17:19:06,842 : INFO : PROGRESS: pass 2, at document #175/175\n",
      "2022-05-03 17:19:06,860 : INFO : topic #0 (0.250): 0.010*\"love\" + 0.009*\"allah\" + 0.008*\"somebody\" + 0.007*\"heard\" + 0.007*\"thought\" + 0.007*\"feel\" + 0.007*\"brilliant\" + 0.007*\"distance\" + 0.007*\"great\" + 0.007*\"bother\"\n",
      "2022-05-03 17:19:06,861 : INFO : topic #1 (0.250): 0.014*\"ve\" + 0.011*\"knew\" + 0.011*\"happened\" + 0.010*\"world\" + 0.010*\"job\" + 0.008*\"artist\" + 0.007*\"talking\" + 0.007*\"success\" + 0.007*\"looking\" + 0.007*\"anybody\"\n",
      "2022-05-03 17:19:06,862 : INFO : topic #2 (0.250): 0.010*\"question\" + 0.010*\"chemical\" + 0.010*\"divine\" + 0.008*\"idea\" + 0.008*\"tom\" + 0.008*\"engineer\" + 0.008*\"reputation\" + 0.008*\"lit\" + 0.008*\"doomed\" + 0.008*\"example\"\n",
      "2022-05-03 17:19:06,863 : INFO : topic #3 (0.250): 0.016*\"ve\" + 0.014*\"god\" + 0.013*\"allah\" + 0.012*\"dance\" + 0.012*\"started\" + 0.010*\"ancient\" + 0.009*\"applause\" + 0.007*\"believe\" + 0.007*\"source\" + 0.007*\"happen\"\n",
      "2022-05-03 17:19:06,864 : INFO : topic diff=0.050278, rho=0.500000\n",
      "2022-05-03 17:19:06,897 : INFO : -6.994 per-word bound, 127.4 perplexity estimate based on a held-out corpus of 175 documents with 902 words\n",
      "2022-05-03 17:19:06,897 : INFO : PROGRESS: pass 3, at document #175/175\n",
      "2022-05-03 17:19:06,917 : INFO : topic #0 (0.250): 0.010*\"love\" + 0.010*\"allah\" + 0.007*\"somebody\" + 0.007*\"heard\" + 0.007*\"thought\" + 0.007*\"feel\" + 0.007*\"brilliant\" + 0.007*\"distance\" + 0.007*\"bother\" + 0.007*\"great\"\n",
      "2022-05-03 17:19:06,918 : INFO : topic #1 (0.250): 0.014*\"ve\" + 0.011*\"knew\" + 0.011*\"happened\" + 0.010*\"world\" + 0.010*\"job\" + 0.008*\"artist\" + 0.007*\"talking\" + 0.007*\"success\" + 0.007*\"looking\" + 0.007*\"better\"\n",
      "2022-05-03 17:19:06,918 : INFO : topic #2 (0.250): 0.010*\"question\" + 0.010*\"chemical\" + 0.010*\"divine\" + 0.008*\"idea\" + 0.008*\"tom\" + 0.008*\"engineer\" + 0.008*\"reputation\" + 0.008*\"lit\" + 0.008*\"doomed\" + 0.008*\"example\"\n",
      "2022-05-03 17:19:06,919 : INFO : topic #3 (0.250): 0.016*\"ve\" + 0.014*\"god\" + 0.012*\"allah\" + 0.012*\"dance\" + 0.012*\"started\" + 0.010*\"ancient\" + 0.009*\"applause\" + 0.007*\"believe\" + 0.007*\"source\" + 0.007*\"happen\"\n",
      "2022-05-03 17:19:06,920 : INFO : topic diff=0.027088, rho=0.447214\n",
      "2022-05-03 17:19:06,952 : INFO : -6.990 per-word bound, 127.1 perplexity estimate based on a held-out corpus of 175 documents with 902 words\n",
      "2022-05-03 17:19:06,952 : INFO : PROGRESS: pass 4, at document #175/175\n",
      "2022-05-03 17:19:06,971 : INFO : topic #0 (0.250): 0.010*\"love\" + 0.010*\"allah\" + 0.007*\"somebody\" + 0.007*\"thought\" + 0.007*\"feel\" + 0.007*\"heard\" + 0.007*\"brilliant\" + 0.007*\"distance\" + 0.007*\"bother\" + 0.007*\"great\"\n",
      "2022-05-03 17:19:06,972 : INFO : topic #1 (0.250): 0.014*\"ve\" + 0.011*\"knew\" + 0.011*\"happened\" + 0.011*\"world\" + 0.010*\"job\" + 0.008*\"artist\" + 0.007*\"talking\" + 0.007*\"success\" + 0.007*\"looking\" + 0.007*\"make\"\n",
      "2022-05-03 17:19:06,974 : INFO : topic #2 (0.250): 0.010*\"question\" + 0.010*\"chemical\" + 0.010*\"divine\" + 0.008*\"idea\" + 0.008*\"tom\" + 0.008*\"engineer\" + 0.008*\"reputation\" + 0.008*\"lit\" + 0.008*\"doomed\" + 0.008*\"example\"\n",
      "2022-05-03 17:19:06,975 : INFO : topic #3 (0.250): 0.016*\"ve\" + 0.014*\"god\" + 0.012*\"allah\" + 0.012*\"dance\" + 0.012*\"started\" + 0.010*\"ancient\" + 0.010*\"applause\" + 0.007*\"believe\" + 0.007*\"source\" + 0.007*\"happen\"\n",
      "2022-05-03 17:19:06,976 : INFO : topic diff=0.015384, rho=0.408248\n",
      "2022-05-03 17:19:07,016 : INFO : -6.989 per-word bound, 127.0 perplexity estimate based on a held-out corpus of 175 documents with 902 words\n",
      "2022-05-03 17:19:07,017 : INFO : PROGRESS: pass 5, at document #175/175\n",
      "2022-05-03 17:19:07,036 : INFO : topic #0 (0.250): 0.010*\"love\" + 0.010*\"allah\" + 0.007*\"thought\" + 0.007*\"feel\" + 0.007*\"somebody\" + 0.007*\"heard\" + 0.007*\"brilliant\" + 0.007*\"distance\" + 0.007*\"bother\" + 0.007*\"great\"\n",
      "2022-05-03 17:19:07,037 : INFO : topic #1 (0.250): 0.014*\"ve\" + 0.011*\"knew\" + 0.011*\"happened\" + 0.011*\"job\" + 0.011*\"world\" + 0.008*\"artist\" + 0.007*\"talking\" + 0.007*\"success\" + 0.007*\"looking\" + 0.007*\"make\"\n",
      "2022-05-03 17:19:07,037 : INFO : topic #2 (0.250): 0.010*\"question\" + 0.010*\"chemical\" + 0.010*\"divine\" + 0.008*\"idea\" + 0.008*\"tom\" + 0.008*\"engineer\" + 0.008*\"reputation\" + 0.008*\"lit\" + 0.008*\"doomed\" + 0.008*\"example\"\n",
      "2022-05-03 17:19:07,037 : INFO : topic #3 (0.250): 0.016*\"ve\" + 0.014*\"god\" + 0.012*\"allah\" + 0.012*\"dance\" + 0.012*\"started\" + 0.010*\"ancient\" + 0.010*\"applause\" + 0.007*\"believe\" + 0.007*\"completely\" + 0.007*\"source\"\n",
      "2022-05-03 17:19:07,038 : INFO : topic diff=0.009084, rho=0.377964\n",
      "2022-05-03 17:19:07,070 : INFO : -6.988 per-word bound, 127.0 perplexity estimate based on a held-out corpus of 175 documents with 902 words\n",
      "2022-05-03 17:19:07,071 : INFO : PROGRESS: pass 6, at document #175/175\n",
      "2022-05-03 17:19:07,094 : INFO : topic #0 (0.250): 0.010*\"love\" + 0.010*\"allah\" + 0.007*\"thought\" + 0.007*\"feel\" + 0.007*\"somebody\" + 0.007*\"brilliant\" + 0.007*\"heard\" + 0.007*\"distance\" + 0.007*\"bother\" + 0.007*\"great\"\n",
      "2022-05-03 17:19:07,095 : INFO : topic #1 (0.250): 0.014*\"ve\" + 0.011*\"knew\" + 0.011*\"job\" + 0.011*\"happened\" + 0.011*\"world\" + 0.007*\"artist\" + 0.007*\"talking\" + 0.007*\"success\" + 0.007*\"looking\" + 0.007*\"make\"\n",
      "2022-05-03 17:19:07,096 : INFO : topic #2 (0.250): 0.010*\"question\" + 0.010*\"divine\" + 0.010*\"chemical\" + 0.008*\"artist\" + 0.008*\"idea\" + 0.008*\"tom\" + 0.008*\"engineer\" + 0.008*\"reputation\" + 0.008*\"lit\" + 0.008*\"doomed\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 17:19:07,097 : INFO : topic #3 (0.250): 0.017*\"ve\" + 0.014*\"god\" + 0.012*\"allah\" + 0.012*\"dance\" + 0.012*\"started\" + 0.010*\"ancient\" + 0.010*\"applause\" + 0.007*\"completely\" + 0.007*\"believe\" + 0.007*\"source\"\n",
      "2022-05-03 17:19:07,098 : INFO : topic diff=0.005546, rho=0.353553\n",
      "2022-05-03 17:19:07,130 : INFO : -6.988 per-word bound, 127.0 perplexity estimate based on a held-out corpus of 175 documents with 902 words\n",
      "2022-05-03 17:19:07,131 : INFO : PROGRESS: pass 7, at document #175/175\n",
      "2022-05-03 17:19:07,150 : INFO : topic #0 (0.250): 0.010*\"love\" + 0.010*\"allah\" + 0.007*\"thought\" + 0.007*\"feel\" + 0.007*\"brilliant\" + 0.007*\"somebody\" + 0.007*\"heard\" + 0.007*\"hand\" + 0.007*\"distance\" + 0.007*\"bother\"\n",
      "2022-05-03 17:19:07,151 : INFO : topic #1 (0.250): 0.014*\"ve\" + 0.011*\"knew\" + 0.011*\"job\" + 0.011*\"happened\" + 0.011*\"world\" + 0.007*\"artist\" + 0.007*\"talking\" + 0.007*\"success\" + 0.007*\"looking\" + 0.007*\"make\"\n",
      "2022-05-03 17:19:07,152 : INFO : topic #2 (0.250): 0.010*\"question\" + 0.010*\"divine\" + 0.010*\"chemical\" + 0.008*\"artist\" + 0.008*\"idea\" + 0.008*\"tom\" + 0.008*\"engineer\" + 0.008*\"reputation\" + 0.008*\"house\" + 0.008*\"lit\"\n",
      "2022-05-03 17:19:07,152 : INFO : topic #3 (0.250): 0.017*\"ve\" + 0.014*\"god\" + 0.012*\"allah\" + 0.012*\"dance\" + 0.012*\"started\" + 0.010*\"ancient\" + 0.010*\"applause\" + 0.007*\"completely\" + 0.007*\"believe\" + 0.007*\"source\"\n",
      "2022-05-03 17:19:07,153 : INFO : topic diff=0.003489, rho=0.333333\n",
      "2022-05-03 17:19:07,186 : INFO : -6.988 per-word bound, 126.9 perplexity estimate based on a held-out corpus of 175 documents with 902 words\n",
      "2022-05-03 17:19:07,186 : INFO : PROGRESS: pass 8, at document #175/175\n",
      "2022-05-03 17:19:07,206 : INFO : topic #0 (0.250): 0.010*\"allah\" + 0.010*\"love\" + 0.007*\"thought\" + 0.007*\"feel\" + 0.007*\"brilliant\" + 0.007*\"century\" + 0.007*\"hand\" + 0.007*\"somebody\" + 0.007*\"heard\" + 0.007*\"distance\"\n",
      "2022-05-03 17:19:07,207 : INFO : topic #1 (0.250): 0.014*\"ve\" + 0.011*\"job\" + 0.011*\"knew\" + 0.011*\"happened\" + 0.011*\"world\" + 0.007*\"talking\" + 0.007*\"artist\" + 0.007*\"success\" + 0.007*\"make\" + 0.007*\"looking\"\n",
      "2022-05-03 17:19:07,207 : INFO : topic #2 (0.250): 0.010*\"question\" + 0.010*\"divine\" + 0.010*\"chemical\" + 0.008*\"artist\" + 0.008*\"idea\" + 0.008*\"house\" + 0.008*\"tom\" + 0.008*\"engineer\" + 0.008*\"reputation\" + 0.008*\"lit\"\n",
      "2022-05-03 17:19:07,208 : INFO : topic #3 (0.250): 0.017*\"ve\" + 0.014*\"god\" + 0.012*\"allah\" + 0.012*\"dance\" + 0.012*\"started\" + 0.010*\"ancient\" + 0.010*\"applause\" + 0.007*\"completely\" + 0.007*\"believe\" + 0.007*\"source\"\n",
      "2022-05-03 17:19:07,209 : INFO : topic diff=0.002254, rho=0.316228\n",
      "2022-05-03 17:19:07,239 : INFO : -6.988 per-word bound, 126.9 perplexity estimate based on a held-out corpus of 175 documents with 902 words\n",
      "2022-05-03 17:19:07,240 : INFO : PROGRESS: pass 9, at document #175/175\n",
      "2022-05-03 17:19:07,259 : INFO : topic #0 (0.250): 0.010*\"allah\" + 0.010*\"love\" + 0.007*\"thought\" + 0.007*\"feel\" + 0.007*\"century\" + 0.007*\"brilliant\" + 0.007*\"hand\" + 0.007*\"heard\" + 0.007*\"somebody\" + 0.007*\"distance\"\n",
      "2022-05-03 17:19:07,260 : INFO : topic #1 (0.250): 0.014*\"ve\" + 0.011*\"job\" + 0.011*\"knew\" + 0.011*\"happened\" + 0.011*\"world\" + 0.007*\"talking\" + 0.007*\"artist\" + 0.007*\"success\" + 0.007*\"make\" + 0.007*\"looking\"\n",
      "2022-05-03 17:19:07,260 : INFO : topic #2 (0.250): 0.010*\"question\" + 0.010*\"divine\" + 0.010*\"chemical\" + 0.008*\"artist\" + 0.008*\"idea\" + 0.008*\"house\" + 0.008*\"tom\" + 0.008*\"engineer\" + 0.008*\"reputation\" + 0.008*\"lit\"\n",
      "2022-05-03 17:19:07,261 : INFO : topic #3 (0.250): 0.017*\"ve\" + 0.014*\"god\" + 0.012*\"allah\" + 0.012*\"dance\" + 0.012*\"started\" + 0.010*\"ancient\" + 0.010*\"applause\" + 0.007*\"completely\" + 0.007*\"believe\" + 0.007*\"source\"\n",
      "2022-05-03 17:19:07,262 : INFO : topic diff=0.001492, rho=0.301511\n",
      "2022-05-03 17:19:07,262 : INFO : topic #0 (0.250): 0.010*\"allah\" + 0.010*\"love\" + 0.007*\"thought\" + 0.007*\"feel\" + 0.007*\"century\" + 0.007*\"brilliant\" + 0.007*\"hand\" + 0.007*\"heard\" + 0.007*\"somebody\" + 0.007*\"distance\"\n",
      "2022-05-03 17:19:07,263 : INFO : topic #1 (0.250): 0.014*\"ve\" + 0.011*\"job\" + 0.011*\"knew\" + 0.011*\"happened\" + 0.011*\"world\" + 0.007*\"talking\" + 0.007*\"artist\" + 0.007*\"success\" + 0.007*\"make\" + 0.007*\"looking\"\n",
      "2022-05-03 17:19:07,263 : INFO : topic #2 (0.250): 0.010*\"question\" + 0.010*\"divine\" + 0.010*\"chemical\" + 0.008*\"artist\" + 0.008*\"idea\" + 0.008*\"house\" + 0.008*\"tom\" + 0.008*\"engineer\" + 0.008*\"reputation\" + 0.008*\"lit\"\n",
      "2022-05-03 17:19:07,264 : INFO : topic #3 (0.250): 0.017*\"ve\" + 0.014*\"god\" + 0.012*\"allah\" + 0.012*\"dance\" + 0.012*\"started\" + 0.010*\"ancient\" + 0.010*\"applause\" + 0.007*\"completely\" + 0.007*\"believe\" + 0.007*\"source\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.010*\"allah\" + 0.010*\"love\" + 0.007*\"thought\" + 0.007*\"feel\" + 0.007*\"century\" + 0.007*\"brilliant\" + 0.007*\"hand\" + 0.007*\"heard\" + 0.007*\"somebody\" + 0.007*\"distance\"'),\n",
       " (1,\n",
       "  '0.014*\"ve\" + 0.011*\"job\" + 0.011*\"knew\" + 0.011*\"happened\" + 0.011*\"world\" + 0.007*\"talking\" + 0.007*\"artist\" + 0.007*\"success\" + 0.007*\"make\" + 0.007*\"looking\"'),\n",
       " (2,\n",
       "  '0.010*\"question\" + 0.010*\"divine\" + 0.010*\"chemical\" + 0.008*\"artist\" + 0.008*\"idea\" + 0.008*\"house\" + 0.008*\"tom\" + 0.008*\"engineer\" + 0.008*\"reputation\" + 0.008*\"lit\"'),\n",
       " (3,\n",
       "  '0.017*\"ve\" + 0.014*\"god\" + 0.012*\"allah\" + 0.012*\"dance\" + 0.012*\"started\" + 0.010*\"ancient\" + 0.010*\"applause\" + 0.007*\"completely\" + 0.007*\"believe\" + 0.007*\"source\"')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f4b6dba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "77935955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i writer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>writing book profession s  course</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it also great lifelong love fascination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and i nt expect s ever going change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>but  said  something kind peculiar happened recently life career  caused recalibrate whole relationship work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>nonetheless  sheer human love stubbornness keep showing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>thank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>applause   thank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>applause   june cohen  olé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>applause</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                        transcript\n",
       "0                                                                                                        i writer \n",
       "1                                                                               writing book profession s  course \n",
       "2                                                                         it also great lifelong love fascination \n",
       "3                                                                             and i nt expect s ever going change \n",
       "4    but  said  something kind peculiar happened recently life career  caused recalibrate whole relationship work \n",
       "..                                                                                                             ...\n",
       "170                                                       nonetheless  sheer human love stubbornness keep showing \n",
       "171                                                                                                         thank \n",
       "172                                                                                              applause   thank \n",
       "173                                                                                    applause   june cohen  olé \n",
       "174                                                                                                      applause \n",
       "\n",
       "[175 rows x 1 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "02b8af06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>writer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>book profession s course</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love fascination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nt change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>something kind life career relationship work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>sheer love stubbornness showing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>thank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>applause thank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>applause june cohen olé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>applause</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       transcript\n",
       "0                                          writer\n",
       "1                        book profession s course\n",
       "2                                love fascination\n",
       "3                                       nt change\n",
       "4    something kind life career relationship work\n",
       "..                                            ...\n",
       "170               sheer love stubbornness showing\n",
       "171                                         thank\n",
       "172                                applause thank\n",
       "173                       applause june cohen olé\n",
       "174                                      applause\n",
       "\n",
       "[175 rows x 1 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "009ca2b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>afar</th>\n",
       "      <th>afraid</th>\n",
       "      <th>air</th>\n",
       "      <th>allah</th>\n",
       "      <th>ancient</th>\n",
       "      <th>angeles</th>\n",
       "      <th>anguish</th>\n",
       "      <th>answer</th>\n",
       "      <th>anxiety</th>\n",
       "      <th>anybody</th>\n",
       "      <th>...</th>\n",
       "      <th>way</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>wonderment</th>\n",
       "      <th>word</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>worried</th>\n",
       "      <th>writer</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175 rows × 292 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     afar  afraid  air  allah  ancient  angeles  anguish  answer  anxiety  \\\n",
       "0       0       0    0      0        0        0        0       0        0   \n",
       "1       0       0    0      0        0        0        0       0        0   \n",
       "2       0       0    0      0        0        0        0       0        0   \n",
       "3       0       0    0      0        0        0        0       0        0   \n",
       "4       0       0    0      0        0        0        0       0        0   \n",
       "..    ...     ...  ...    ...      ...      ...      ...     ...      ...   \n",
       "170     0       0    0      0        0        0        0       0        0   \n",
       "171     0       0    0      0        0        0        0       0        0   \n",
       "172     0       0    0      0        0        0        0       0        0   \n",
       "173     0       0    0      0        0        0        0       0        0   \n",
       "174     0       0    0      0        0        0        0       0        0   \n",
       "\n",
       "     anybody  ...  way  wisdom  wonderment  word  work  world  worried  \\\n",
       "0          0  ...    0       0           0     0     0      0        0   \n",
       "1          0  ...    0       0           0     0     0      0        0   \n",
       "2          0  ...    0       0           0     0     0      0        0   \n",
       "3          0  ...    0       0           0     0     0      0        0   \n",
       "4          0  ...    0       0           0     0     1      0        0   \n",
       "..       ...  ...  ...     ...         ...   ...   ...    ...      ...   \n",
       "170        0  ...    0       0           0     0     0      0        0   \n",
       "171        0  ...    0       0           0     0     0      0        0   \n",
       "172        0  ...    0       0           0     0     0      0        0   \n",
       "173        0  ...    0       0           0     0     0      0        0   \n",
       "174        0  ...    0       0           0     0     0      0        0   \n",
       "\n",
       "     writer  year  yes  \n",
       "0         1     0    0  \n",
       "1         0     0    0  \n",
       "2         0     0    0  \n",
       "3         0     0    0  \n",
       "4         0     0    0  \n",
       "..      ...   ...  ...  \n",
       "170       0     0    0  \n",
       "171       0     0    0  \n",
       "172       0     0    0  \n",
       "173       0     0    0  \n",
       "174       0     0    0  \n",
       "\n",
       "[175 rows x 292 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d6e9ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "32882e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 17:19:07,636 : INFO : using symmetric alpha at 0.25\n",
      "2022-05-03 17:19:07,638 : INFO : using symmetric eta at 0.25\n",
      "2022-05-03 17:19:07,639 : INFO : using serial LDA version on this node\n",
      "2022-05-03 17:19:07,640 : INFO : running online (multi-pass) LDA training, 4 topics, 10 passes over the supplied corpus of 175 documents, updating model once every 175 documents, evaluating perplexity every 175 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-05-03 17:19:07,702 : INFO : -7.808 per-word bound, 224.1 perplexity estimate based on a held-out corpus of 175 documents with 567 words\n",
      "2022-05-03 17:19:07,703 : INFO : PROGRESS: pass 0, at document #175/175\n",
      "2022-05-03 17:19:07,751 : INFO : topic #0 (0.250): 0.044*\"work\" + 0.020*\"creativity\" + 0.020*\"laughter\" + 0.016*\"kind\" + 0.016*\"year\" + 0.016*\"page\" + 0.015*\"feel\" + 0.015*\"applause\" + 0.011*\"thing\" + 0.011*\"way\"\n",
      "2022-05-03 17:19:07,752 : INFO : topic #1 (0.250): 0.033*\"sort\" + 0.025*\"god\" + 0.019*\"life\" + 0.019*\"source\" + 0.015*\"allah\" + 0.015*\"chemical\" + 0.015*\"way\" + 0.015*\"process\" + 0.015*\"nt\" + 0.015*\"laughter\"\n",
      "2022-05-03 17:19:07,752 : INFO : topic #2 (0.250): 0.030*\"book\" + 0.026*\"dance\" + 0.026*\"thing\" + 0.025*\"olé\" + 0.022*\"century\" + 0.018*\"way\" + 0.017*\"life\" + 0.017*\"allah\" + 0.017*\"job\" + 0.013*\"nt\"\n",
      "2022-05-03 17:19:07,753 : INFO : topic #3 (0.250): 0.032*\"thing\" + 0.032*\"work\" + 0.031*\"kind\" + 0.031*\"genius\" + 0.022*\"idea\" + 0.021*\"sort\" + 0.021*\"process\" + 0.016*\"artist\" + 0.016*\"somebody\" + 0.012*\"life\"\n",
      "2022-05-03 17:19:07,753 : INFO : topic diff=2.415701, rho=1.000000\n",
      "2022-05-03 17:19:07,785 : INFO : -6.251 per-word bound, 76.1 perplexity estimate based on a held-out corpus of 175 documents with 567 words\n",
      "2022-05-03 17:19:07,786 : INFO : PROGRESS: pass 1, at document #175/175\n",
      "2022-05-03 17:19:07,808 : INFO : topic #0 (0.250): 0.044*\"work\" + 0.023*\"laughter\" + 0.020*\"creativity\" + 0.016*\"year\" + 0.016*\"kind\" + 0.015*\"page\" + 0.015*\"feel\" + 0.015*\"applause\" + 0.011*\"thing\" + 0.011*\"way\"\n",
      "2022-05-03 17:19:07,809 : INFO : topic #1 (0.250): 0.034*\"sort\" + 0.025*\"god\" + 0.020*\"life\" + 0.020*\"source\" + 0.016*\"nt\" + 0.015*\"allah\" + 0.015*\"chemical\" + 0.015*\"way\" + 0.015*\"question\" + 0.012*\"laughter\"\n",
      "2022-05-03 17:19:07,810 : INFO : topic #2 (0.250): 0.033*\"book\" + 0.028*\"olé\" + 0.026*\"thing\" + 0.026*\"dance\" + 0.022*\"century\" + 0.018*\"way\" + 0.018*\"life\" + 0.018*\"allah\" + 0.017*\"job\" + 0.013*\"success\"\n",
      "2022-05-03 17:19:07,811 : INFO : topic #3 (0.250): 0.032*\"work\" + 0.032*\"kind\" + 0.032*\"genius\" + 0.031*\"thing\" + 0.024*\"process\" + 0.022*\"idea\" + 0.022*\"sort\" + 0.017*\"artist\" + 0.016*\"somebody\" + 0.012*\"landscape\"\n",
      "2022-05-03 17:19:07,811 : INFO : topic diff=0.073281, rho=0.577350\n",
      "2022-05-03 17:19:07,842 : INFO : -6.233 per-word bound, 75.2 perplexity estimate based on a held-out corpus of 175 documents with 567 words\n",
      "2022-05-03 17:19:07,842 : INFO : PROGRESS: pass 2, at document #175/175\n",
      "2022-05-03 17:19:07,862 : INFO : topic #0 (0.250): 0.044*\"work\" + 0.024*\"laughter\" + 0.021*\"creativity\" + 0.016*\"year\" + 0.016*\"kind\" + 0.015*\"page\" + 0.015*\"feel\" + 0.015*\"applause\" + 0.011*\"way\" + 0.011*\"poem\"\n",
      "2022-05-03 17:19:07,863 : INFO : topic #1 (0.250): 0.034*\"sort\" + 0.025*\"god\" + 0.020*\"life\" + 0.020*\"source\" + 0.018*\"nt\" + 0.015*\"allah\" + 0.015*\"way\" + 0.015*\"chemical\" + 0.015*\"question\" + 0.011*\"laughter\"\n",
      "2022-05-03 17:19:07,863 : INFO : topic #2 (0.250): 0.035*\"book\" + 0.029*\"olé\" + 0.027*\"thing\" + 0.026*\"dance\" + 0.022*\"century\" + 0.018*\"way\" + 0.018*\"life\" + 0.018*\"allah\" + 0.018*\"job\" + 0.014*\"success\"\n",
      "2022-05-03 17:19:07,864 : INFO : topic #3 (0.250): 0.032*\"kind\" + 0.032*\"genius\" + 0.032*\"work\" + 0.030*\"thing\" + 0.026*\"process\" + 0.022*\"idea\" + 0.022*\"sort\" + 0.017*\"artist\" + 0.016*\"somebody\" + 0.012*\"question\"\n",
      "2022-05-03 17:19:07,865 : INFO : topic diff=0.032063, rho=0.500000\n",
      "2022-05-03 17:19:07,893 : INFO : -6.227 per-word bound, 74.9 perplexity estimate based on a held-out corpus of 175 documents with 567 words\n",
      "2022-05-03 17:19:07,894 : INFO : PROGRESS: pass 3, at document #175/175\n",
      "2022-05-03 17:19:07,914 : INFO : topic #0 (0.250): 0.044*\"work\" + 0.026*\"laughter\" + 0.021*\"creativity\" + 0.016*\"year\" + 0.015*\"kind\" + 0.015*\"page\" + 0.015*\"feel\" + 0.015*\"applause\" + 0.011*\"way\" + 0.011*\"poem\"\n",
      "2022-05-03 17:19:07,915 : INFO : topic #1 (0.250): 0.034*\"sort\" + 0.025*\"god\" + 0.020*\"life\" + 0.020*\"source\" + 0.019*\"nt\" + 0.015*\"allah\" + 0.015*\"way\" + 0.015*\"chemical\" + 0.015*\"question\" + 0.011*\"love\"\n",
      "2022-05-03 17:19:07,916 : INFO : topic #2 (0.250): 0.036*\"book\" + 0.030*\"olé\" + 0.027*\"thing\" + 0.026*\"dance\" + 0.022*\"century\" + 0.018*\"life\" + 0.018*\"allah\" + 0.018*\"way\" + 0.018*\"job\" + 0.014*\"success\"\n",
      "2022-05-03 17:19:07,917 : INFO : topic #3 (0.250): 0.032*\"kind\" + 0.032*\"genius\" + 0.032*\"work\" + 0.030*\"thing\" + 0.028*\"process\" + 0.022*\"idea\" + 0.022*\"sort\" + 0.017*\"artist\" + 0.016*\"somebody\" + 0.012*\"question\"\n",
      "2022-05-03 17:19:07,917 : INFO : topic diff=0.018137, rho=0.447214\n",
      "2022-05-03 17:19:07,947 : INFO : -6.224 per-word bound, 74.7 perplexity estimate based on a held-out corpus of 175 documents with 567 words\n",
      "2022-05-03 17:19:07,948 : INFO : PROGRESS: pass 4, at document #175/175\n",
      "2022-05-03 17:19:07,969 : INFO : topic #0 (0.250): 0.044*\"work\" + 0.028*\"laughter\" + 0.022*\"creativity\" + 0.016*\"year\" + 0.015*\"kind\" + 0.015*\"page\" + 0.015*\"feel\" + 0.015*\"applause\" + 0.011*\"way\" + 0.011*\"poem\"\n",
      "2022-05-03 17:19:07,969 : INFO : topic #1 (0.250): 0.034*\"sort\" + 0.025*\"god\" + 0.020*\"life\" + 0.020*\"source\" + 0.019*\"nt\" + 0.015*\"allah\" + 0.015*\"way\" + 0.015*\"chemical\" + 0.015*\"question\" + 0.011*\"love\"\n",
      "2022-05-03 17:19:07,970 : INFO : topic #2 (0.250): 0.037*\"book\" + 0.030*\"olé\" + 0.028*\"thing\" + 0.026*\"dance\" + 0.022*\"century\" + 0.018*\"life\" + 0.018*\"allah\" + 0.018*\"job\" + 0.018*\"way\" + 0.014*\"success\"\n",
      "2022-05-03 17:19:07,970 : INFO : topic #3 (0.250): 0.032*\"kind\" + 0.032*\"genius\" + 0.032*\"work\" + 0.030*\"process\" + 0.030*\"thing\" + 0.022*\"idea\" + 0.022*\"sort\" + 0.017*\"artist\" + 0.016*\"somebody\" + 0.012*\"question\"\n",
      "2022-05-03 17:19:07,971 : INFO : topic diff=0.010774, rho=0.408248\n",
      "2022-05-03 17:19:08,005 : INFO : -6.222 per-word bound, 74.6 perplexity estimate based on a held-out corpus of 175 documents with 567 words\n",
      "2022-05-03 17:19:08,006 : INFO : PROGRESS: pass 5, at document #175/175\n",
      "2022-05-03 17:19:08,030 : INFO : topic #0 (0.250): 0.044*\"work\" + 0.028*\"laughter\" + 0.023*\"creativity\" + 0.016*\"year\" + 0.015*\"kind\" + 0.015*\"page\" + 0.015*\"feel\" + 0.015*\"applause\" + 0.011*\"poem\" + 0.011*\"way\"\n",
      "2022-05-03 17:19:08,031 : INFO : topic #1 (0.250): 0.035*\"sort\" + 0.025*\"god\" + 0.020*\"life\" + 0.020*\"source\" + 0.020*\"nt\" + 0.015*\"way\" + 0.015*\"allah\" + 0.015*\"chemical\" + 0.015*\"question\" + 0.011*\"love\"\n",
      "2022-05-03 17:19:08,031 : INFO : topic #2 (0.250): 0.037*\"book\" + 0.031*\"olé\" + 0.028*\"thing\" + 0.026*\"dance\" + 0.022*\"century\" + 0.018*\"life\" + 0.018*\"allah\" + 0.018*\"job\" + 0.018*\"way\" + 0.014*\"success\"\n",
      "2022-05-03 17:19:08,032 : INFO : topic #3 (0.250): 0.032*\"kind\" + 0.032*\"genius\" + 0.032*\"work\" + 0.031*\"process\" + 0.029*\"thing\" + 0.022*\"idea\" + 0.022*\"sort\" + 0.017*\"artist\" + 0.016*\"somebody\" + 0.012*\"question\"\n",
      "2022-05-03 17:19:08,032 : INFO : topic diff=0.006910, rho=0.377964\n",
      "2022-05-03 17:19:08,066 : INFO : -6.221 per-word bound, 74.6 perplexity estimate based on a held-out corpus of 175 documents with 567 words\n",
      "2022-05-03 17:19:08,067 : INFO : PROGRESS: pass 6, at document #175/175\n",
      "2022-05-03 17:19:08,087 : INFO : topic #0 (0.250): 0.044*\"work\" + 0.030*\"laughter\" + 0.023*\"creativity\" + 0.016*\"year\" + 0.015*\"kind\" + 0.015*\"page\" + 0.015*\"feel\" + 0.015*\"applause\" + 0.011*\"poem\" + 0.011*\"way\"\n",
      "2022-05-03 17:19:08,087 : INFO : topic #1 (0.250): 0.035*\"sort\" + 0.025*\"god\" + 0.020*\"life\" + 0.020*\"source\" + 0.020*\"nt\" + 0.016*\"way\" + 0.015*\"chemical\" + 0.015*\"allah\" + 0.015*\"question\" + 0.011*\"love\"\n",
      "2022-05-03 17:19:08,088 : INFO : topic #2 (0.250): 0.038*\"book\" + 0.032*\"olé\" + 0.028*\"thing\" + 0.026*\"dance\" + 0.022*\"century\" + 0.018*\"allah\" + 0.018*\"life\" + 0.018*\"job\" + 0.018*\"way\" + 0.014*\"success\"\n",
      "2022-05-03 17:19:08,088 : INFO : topic #3 (0.250): 0.032*\"genius\" + 0.032*\"process\" + 0.032*\"kind\" + 0.032*\"work\" + 0.029*\"thing\" + 0.022*\"idea\" + 0.022*\"sort\" + 0.017*\"artist\" + 0.016*\"somebody\" + 0.012*\"question\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 17:19:08,089 : INFO : topic diff=0.005994, rho=0.353553\n",
      "2022-05-03 17:19:08,122 : INFO : -6.219 per-word bound, 74.5 perplexity estimate based on a held-out corpus of 175 documents with 567 words\n",
      "2022-05-03 17:19:08,123 : INFO : PROGRESS: pass 7, at document #175/175\n",
      "2022-05-03 17:19:08,143 : INFO : topic #0 (0.250): 0.044*\"work\" + 0.031*\"laughter\" + 0.024*\"creativity\" + 0.016*\"year\" + 0.015*\"kind\" + 0.015*\"page\" + 0.015*\"feel\" + 0.015*\"applause\" + 0.011*\"poem\" + 0.011*\"way\"\n",
      "2022-05-03 17:19:08,143 : INFO : topic #1 (0.250): 0.035*\"sort\" + 0.025*\"god\" + 0.020*\"life\" + 0.020*\"source\" + 0.020*\"nt\" + 0.016*\"way\" + 0.016*\"chemical\" + 0.016*\"allah\" + 0.015*\"question\" + 0.011*\"love\"\n",
      "2022-05-03 17:19:08,144 : INFO : topic #2 (0.250): 0.038*\"book\" + 0.032*\"olé\" + 0.028*\"thing\" + 0.026*\"dance\" + 0.022*\"century\" + 0.018*\"allah\" + 0.018*\"life\" + 0.018*\"job\" + 0.018*\"way\" + 0.014*\"success\"\n",
      "2022-05-03 17:19:08,145 : INFO : topic #3 (0.250): 0.033*\"process\" + 0.033*\"genius\" + 0.032*\"kind\" + 0.032*\"work\" + 0.029*\"thing\" + 0.022*\"idea\" + 0.021*\"sort\" + 0.017*\"artist\" + 0.016*\"somebody\" + 0.012*\"moment\"\n",
      "2022-05-03 17:19:08,146 : INFO : topic diff=0.005144, rho=0.333333\n",
      "2022-05-03 17:19:08,177 : INFO : -6.218 per-word bound, 74.4 perplexity estimate based on a held-out corpus of 175 documents with 567 words\n",
      "2022-05-03 17:19:08,178 : INFO : PROGRESS: pass 8, at document #175/175\n",
      "2022-05-03 17:19:08,197 : INFO : topic #0 (0.250): 0.044*\"work\" + 0.032*\"laughter\" + 0.024*\"creativity\" + 0.015*\"year\" + 0.015*\"kind\" + 0.015*\"page\" + 0.015*\"feel\" + 0.015*\"applause\" + 0.011*\"poem\" + 0.011*\"way\"\n",
      "2022-05-03 17:19:08,198 : INFO : topic #1 (0.250): 0.035*\"sort\" + 0.025*\"god\" + 0.020*\"life\" + 0.020*\"source\" + 0.020*\"nt\" + 0.016*\"way\" + 0.016*\"chemical\" + 0.016*\"allah\" + 0.015*\"question\" + 0.011*\"love\"\n",
      "2022-05-03 17:19:08,198 : INFO : topic #2 (0.250): 0.038*\"book\" + 0.033*\"olé\" + 0.029*\"thing\" + 0.026*\"dance\" + 0.022*\"century\" + 0.018*\"allah\" + 0.018*\"life\" + 0.018*\"job\" + 0.018*\"way\" + 0.014*\"success\"\n",
      "2022-05-03 17:19:08,199 : INFO : topic #3 (0.250): 0.034*\"process\" + 0.033*\"genius\" + 0.032*\"kind\" + 0.032*\"work\" + 0.028*\"thing\" + 0.022*\"idea\" + 0.021*\"sort\" + 0.017*\"artist\" + 0.016*\"somebody\" + 0.012*\"moment\"\n",
      "2022-05-03 17:19:08,200 : INFO : topic diff=0.004342, rho=0.316228\n",
      "2022-05-03 17:19:08,233 : INFO : -6.216 per-word bound, 74.3 perplexity estimate based on a held-out corpus of 175 documents with 567 words\n",
      "2022-05-03 17:19:08,233 : INFO : PROGRESS: pass 9, at document #175/175\n",
      "2022-05-03 17:19:08,252 : INFO : topic #0 (0.250): 0.044*\"work\" + 0.032*\"laughter\" + 0.024*\"creativity\" + 0.015*\"year\" + 0.015*\"page\" + 0.015*\"kind\" + 0.015*\"feel\" + 0.015*\"applause\" + 0.011*\"poem\" + 0.011*\"way\"\n",
      "2022-05-03 17:19:08,253 : INFO : topic #1 (0.250): 0.035*\"sort\" + 0.025*\"god\" + 0.021*\"life\" + 0.020*\"source\" + 0.020*\"nt\" + 0.016*\"way\" + 0.016*\"chemical\" + 0.016*\"allah\" + 0.015*\"question\" + 0.011*\"love\"\n",
      "2022-05-03 17:19:08,253 : INFO : topic #2 (0.250): 0.038*\"book\" + 0.033*\"olé\" + 0.031*\"thing\" + 0.026*\"dance\" + 0.022*\"century\" + 0.018*\"allah\" + 0.018*\"life\" + 0.018*\"job\" + 0.018*\"way\" + 0.013*\"success\"\n",
      "2022-05-03 17:19:08,254 : INFO : topic #3 (0.250): 0.035*\"process\" + 0.034*\"genius\" + 0.032*\"kind\" + 0.032*\"work\" + 0.025*\"thing\" + 0.022*\"idea\" + 0.021*\"sort\" + 0.017*\"artist\" + 0.016*\"somebody\" + 0.012*\"moment\"\n",
      "2022-05-03 17:19:08,254 : INFO : topic diff=0.004028, rho=0.301511\n",
      "2022-05-03 17:19:08,255 : INFO : topic #0 (0.250): 0.044*\"work\" + 0.032*\"laughter\" + 0.024*\"creativity\" + 0.015*\"year\" + 0.015*\"page\" + 0.015*\"kind\" + 0.015*\"feel\" + 0.015*\"applause\" + 0.011*\"poem\" + 0.011*\"way\"\n",
      "2022-05-03 17:19:08,256 : INFO : topic #1 (0.250): 0.035*\"sort\" + 0.025*\"god\" + 0.021*\"life\" + 0.020*\"source\" + 0.020*\"nt\" + 0.016*\"way\" + 0.016*\"chemical\" + 0.016*\"allah\" + 0.015*\"question\" + 0.011*\"love\"\n",
      "2022-05-03 17:19:08,257 : INFO : topic #2 (0.250): 0.038*\"book\" + 0.033*\"olé\" + 0.031*\"thing\" + 0.026*\"dance\" + 0.022*\"century\" + 0.018*\"allah\" + 0.018*\"life\" + 0.018*\"job\" + 0.018*\"way\" + 0.013*\"success\"\n",
      "2022-05-03 17:19:08,257 : INFO : topic #3 (0.250): 0.035*\"process\" + 0.034*\"genius\" + 0.032*\"kind\" + 0.032*\"work\" + 0.025*\"thing\" + 0.022*\"idea\" + 0.021*\"sort\" + 0.017*\"artist\" + 0.016*\"somebody\" + 0.012*\"moment\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.044*\"work\" + 0.032*\"laughter\" + 0.024*\"creativity\" + 0.015*\"year\" + 0.015*\"page\" + 0.015*\"kind\" + 0.015*\"feel\" + 0.015*\"applause\" + 0.011*\"poem\" + 0.011*\"way\"'),\n",
       " (1,\n",
       "  '0.035*\"sort\" + 0.025*\"god\" + 0.021*\"life\" + 0.020*\"source\" + 0.020*\"nt\" + 0.016*\"way\" + 0.016*\"chemical\" + 0.016*\"allah\" + 0.015*\"question\" + 0.011*\"love\"'),\n",
       " (2,\n",
       "  '0.038*\"book\" + 0.033*\"olé\" + 0.031*\"thing\" + 0.026*\"dance\" + 0.022*\"century\" + 0.018*\"allah\" + 0.018*\"life\" + 0.018*\"job\" + 0.018*\"way\" + 0.013*\"success\"'),\n",
       " (3,\n",
       "  '0.035*\"process\" + 0.034*\"genius\" + 0.032*\"kind\" + 0.032*\"work\" + 0.025*\"thing\" + 0.022*\"idea\" + 0.021*\"sort\" + 0.017*\"artist\" + 0.016*\"somebody\" + 0.012*\"moment\"')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4d643340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "924e6236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i writer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>book profession s course</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>great lifelong love fascination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nt s change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>something kind peculiar life career recalibrate whole relationship work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>sheer human love stubbornness showing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>thank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>applause thank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>applause june cohen olé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>applause</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  transcript\n",
       "0                                                                   i writer\n",
       "1                                                   book profession s course\n",
       "2                                            great lifelong love fascination\n",
       "3                                                                nt s change\n",
       "4    something kind peculiar life career recalibrate whole relationship work\n",
       "..                                                                       ...\n",
       "170                                    sheer human love stubbornness showing\n",
       "171                                                                    thank\n",
       "172                                                           applause thank\n",
       "173                                                  applause june cohen olé\n",
       "174                                                                 applause\n",
       "\n",
       "[175 rows x 1 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f8d43240",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>afar</th>\n",
       "      <th>afraid</th>\n",
       "      <th>africa</th>\n",
       "      <th>air</th>\n",
       "      <th>alcoholic</th>\n",
       "      <th>allah</th>\n",
       "      <th>aloud</th>\n",
       "      <th>american</th>\n",
       "      <th>ancient</th>\n",
       "      <th>...</th>\n",
       "      <th>wondrous</th>\n",
       "      <th>word</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>worried</th>\n",
       "      <th>worst</th>\n",
       "      <th>writer</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175 rows × 441 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     able  afar  afraid  africa  air  alcoholic  allah  aloud  american  \\\n",
       "0       0     0       0       0    0          0      0      0         0   \n",
       "1       0     0       0       0    0          0      0      0         0   \n",
       "2       0     0       0       0    0          0      0      0         0   \n",
       "3       0     0       0       0    0          0      0      0         0   \n",
       "4       0     0       0       0    0          0      0      0         0   \n",
       "..    ...   ...     ...     ...  ...        ...    ...    ...       ...   \n",
       "170     0     0       0       0    0          0      0      0         0   \n",
       "171     0     0       0       0    0          0      0      0         0   \n",
       "172     0     0       0       0    0          0      0      0         0   \n",
       "173     0     0       0       0    0          0      0      0         0   \n",
       "174     0     0       0       0    0          0      0      0         0   \n",
       "\n",
       "     ancient  ...  wondrous  word  work  world  worried  worst  writer  year  \\\n",
       "0          0  ...         0     0     0      0        0      0       1     0   \n",
       "1          0  ...         0     0     0      0        0      0       0     0   \n",
       "2          0  ...         0     0     0      0        0      0       0     0   \n",
       "3          0  ...         0     0     0      0        0      0       0     0   \n",
       "4          0  ...         0     0     1      0        0      0       0     0   \n",
       "..       ...  ...       ...   ...   ...    ...      ...    ...     ...   ...   \n",
       "170        0  ...         0     0     0      0        0      0       0     0   \n",
       "171        0  ...         0     0     0      0        0      0       0     0   \n",
       "172        0  ...         0     0     0      0        0      0       0     0   \n",
       "173        0  ...         0     0     0      0        0      0       0     0   \n",
       "174        0  ...         0     0     0      0        0      0       0     0   \n",
       "\n",
       "     yes  young  \n",
       "0      0      0  \n",
       "1      0      0  \n",
       "2      0      0  \n",
       "3      0      0  \n",
       "4      0      0  \n",
       "..   ...    ...  \n",
       "170    0      0  \n",
       "171    0      0  \n",
       "172    0      0  \n",
       "173    0      0  \n",
       "174    0      0  \n",
       "\n",
       "[175 rows x 441 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1bd222f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4daee25f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 17:19:08,533 : INFO : using symmetric alpha at 0.2\n",
      "2022-05-03 17:19:08,534 : INFO : using symmetric eta at 0.2\n",
      "2022-05-03 17:19:08,535 : INFO : using serial LDA version on this node\n",
      "2022-05-03 17:19:08,537 : INFO : running online (multi-pass) LDA training, 5 topics, 80 passes over the supplied corpus of 175 documents, updating model once every 175 documents, evaluating perplexity every 175 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2022-05-03 17:19:08,603 : INFO : -8.852 per-word bound, 462.0 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:08,603 : INFO : PROGRESS: pass 0, at document #175/175\n",
      "2022-05-03 17:19:08,658 : INFO : topic #0 (0.200): 0.019*\"sort\" + 0.019*\"work\" + 0.019*\"genius\" + 0.018*\"olé\" + 0.015*\"artist\" + 0.013*\"laughter\" + 0.012*\"creative\" + 0.012*\"thing\" + 0.011*\"divine\" + 0.011*\"chemical\"\n",
      "2022-05-03 17:19:08,659 : INFO : topic #1 (0.200): 0.033*\"nt\" + 0.020*\"dance\" + 0.015*\"afraid\" + 0.015*\"work\" + 0.015*\"way\" + 0.013*\"laughter\" + 0.013*\"applause\" + 0.012*\"century\" + 0.010*\"kind\" + 0.010*\"genius\"\n",
      "2022-05-03 17:19:08,659 : INFO : topic #2 (0.200): 0.024*\"creative\" + 0.022*\"nt\" + 0.015*\"idea\" + 0.015*\"god\" + 0.015*\"life\" + 0.015*\"way\" + 0.011*\"sort\" + 0.011*\"allah\" + 0.010*\"source\" + 0.008*\"work\"\n",
      "2022-05-03 17:19:08,660 : INFO : topic #3 (0.200): 0.019*\"work\" + 0.015*\"olé\" + 0.015*\"poem\" + 0.015*\"book\" + 0.015*\"kind\" + 0.015*\"thing\" + 0.012*\"allah\" + 0.011*\"life\" + 0.011*\"ancient\" + 0.011*\"creativity\"\n",
      "2022-05-03 17:19:08,661 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"sort\" + 0.014*\"big\" + 0.013*\"divine\" + 0.013*\"unknowable\" + 0.012*\"human\" + 0.010*\"work\" + 0.010*\"book\" + 0.010*\"distant\" + 0.010*\"reason\"\n",
      "2022-05-03 17:19:08,661 : INFO : topic diff=3.168799, rho=1.000000\n",
      "2022-05-03 17:19:08,695 : INFO : -6.733 per-word bound, 106.4 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:08,695 : INFO : PROGRESS: pass 1, at document #175/175\n",
      "2022-05-03 17:19:08,718 : INFO : topic #0 (0.200): 0.022*\"work\" + 0.022*\"olé\" + 0.021*\"genius\" + 0.018*\"sort\" + 0.016*\"artist\" + 0.015*\"laughter\" + 0.014*\"process\" + 0.012*\"creative\" + 0.011*\"thing\" + 0.011*\"chemical\"\n",
      "2022-05-03 17:19:08,719 : INFO : topic #1 (0.200): 0.035*\"nt\" + 0.022*\"dance\" + 0.016*\"afraid\" + 0.014*\"applause\" + 0.014*\"century\" + 0.012*\"way\" + 0.012*\"work\" + 0.011*\"laughter\" + 0.010*\"kind\" + 0.010*\"dancer\"\n",
      "2022-05-03 17:19:08,720 : INFO : topic #2 (0.200): 0.024*\"nt\" + 0.023*\"creative\" + 0.017*\"god\" + 0.017*\"way\" + 0.015*\"idea\" + 0.015*\"life\" + 0.011*\"sort\" + 0.011*\"allah\" + 0.010*\"piece\" + 0.009*\"source\"\n",
      "2022-05-03 17:19:08,720 : INFO : topic #3 (0.200): 0.017*\"work\" + 0.017*\"book\" + 0.016*\"poem\" + 0.015*\"kind\" + 0.015*\"thing\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:08,721 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"sort\" + 0.015*\"big\" + 0.014*\"divine\" + 0.014*\"unknowable\" + 0.014*\"human\" + 0.013*\"question\" + 0.010*\"book\" + 0.010*\"work\" + 0.010*\"distant\"\n",
      "2022-05-03 17:19:08,721 : INFO : topic diff=0.069389, rho=0.577350\n",
      "2022-05-03 17:19:08,754 : INFO : -6.690 per-word bound, 103.2 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:08,754 : INFO : PROGRESS: pass 2, at document #175/175\n",
      "2022-05-03 17:19:08,774 : INFO : topic #0 (0.200): 0.024*\"work\" + 0.024*\"olé\" + 0.023*\"genius\" + 0.018*\"sort\" + 0.017*\"artist\" + 0.016*\"laughter\" + 0.016*\"process\" + 0.013*\"creative\" + 0.011*\"year\" + 0.011*\"chemical\"\n",
      "2022-05-03 17:19:08,775 : INFO : topic #1 (0.200): 0.036*\"nt\" + 0.023*\"dance\" + 0.017*\"afraid\" + 0.014*\"applause\" + 0.014*\"century\" + 0.011*\"way\" + 0.011*\"work\" + 0.011*\"laughter\" + 0.010*\"kind\" + 0.010*\"dancer\"\n",
      "2022-05-03 17:19:08,775 : INFO : topic #2 (0.200): 0.025*\"nt\" + 0.023*\"creative\" + 0.018*\"god\" + 0.018*\"way\" + 0.015*\"idea\" + 0.015*\"life\" + 0.012*\"sort\" + 0.012*\"allah\" + 0.011*\"piece\" + 0.008*\"source\"\n",
      "2022-05-03 17:19:08,776 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"work\" + 0.016*\"poem\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:08,777 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"sort\" + 0.015*\"big\" + 0.014*\"divine\" + 0.014*\"unknowable\" + 0.014*\"human\" + 0.014*\"question\" + 0.011*\"book\" + 0.010*\"work\" + 0.010*\"distant\"\n",
      "2022-05-03 17:19:08,777 : INFO : topic diff=0.037516, rho=0.500000\n",
      "2022-05-03 17:19:08,807 : INFO : -6.673 per-word bound, 102.1 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:08,807 : INFO : PROGRESS: pass 3, at document #175/175\n",
      "2022-05-03 17:19:08,826 : INFO : topic #0 (0.200): 0.024*\"work\" + 0.024*\"olé\" + 0.024*\"genius\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.017*\"laughter\" + 0.017*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"chemical\"\n",
      "2022-05-03 17:19:08,827 : INFO : topic #1 (0.200): 0.037*\"nt\" + 0.024*\"dance\" + 0.017*\"afraid\" + 0.015*\"century\" + 0.015*\"applause\" + 0.011*\"way\" + 0.011*\"work\" + 0.011*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\"\n",
      "2022-05-03 17:19:08,827 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.018*\"god\" + 0.018*\"way\" + 0.015*\"idea\" + 0.015*\"life\" + 0.012*\"sort\" + 0.012*\"allah\" + 0.011*\"piece\" + 0.008*\"source\"\n",
      "2022-05-03 17:19:08,828 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"work\" + 0.016*\"kind\" + 0.016*\"poem\" + 0.016*\"thing\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:08,829 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"sort\" + 0.015*\"big\" + 0.014*\"divine\" + 0.014*\"unknowable\" + 0.014*\"human\" + 0.014*\"question\" + 0.011*\"book\" + 0.010*\"distant\" + 0.010*\"reason\"\n",
      "2022-05-03 17:19:08,829 : INFO : topic diff=0.022972, rho=0.447214\n",
      "2022-05-03 17:19:08,859 : INFO : -6.667 per-word bound, 101.6 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:08,860 : INFO : PROGRESS: pass 4, at document #175/175\n",
      "2022-05-03 17:19:08,879 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.024*\"olé\" + 0.024*\"genius\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.017*\"laughter\" + 0.017*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"chemical\"\n",
      "2022-05-03 17:19:08,879 : INFO : topic #1 (0.200): 0.037*\"nt\" + 0.024*\"dance\" + 0.018*\"afraid\" + 0.016*\"century\" + 0.015*\"applause\" + 0.011*\"way\" + 0.010*\"work\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\"\n",
      "2022-05-03 17:19:08,880 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"idea\" + 0.015*\"life\" + 0.012*\"sort\" + 0.012*\"allah\" + 0.011*\"piece\" + 0.008*\"source\"\n",
      "2022-05-03 17:19:08,881 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"work\" + 0.016*\"kind\" + 0.016*\"poem\" + 0.016*\"thing\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:08,881 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"sort\" + 0.015*\"big\" + 0.015*\"divine\" + 0.015*\"human\" + 0.015*\"unknowable\" + 0.014*\"question\" + 0.011*\"book\" + 0.010*\"distant\" + 0.010*\"reason\"\n",
      "2022-05-03 17:19:08,882 : INFO : topic diff=0.015181, rho=0.408248\n",
      "2022-05-03 17:19:08,910 : INFO : -6.664 per-word bound, 101.4 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:08,910 : INFO : PROGRESS: pass 5, at document #175/175\n",
      "2022-05-03 17:19:08,929 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"olé\" + 0.024*\"genius\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.017*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"chemical\"\n",
      "2022-05-03 17:19:08,929 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.018*\"afraid\" + 0.017*\"century\" + 0.015*\"applause\" + 0.010*\"way\" + 0.010*\"laughter\" + 0.010*\"work\" + 0.010*\"dancer\" + 0.010*\"kind\"\n",
      "2022-05-03 17:19:08,930 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.011*\"piece\" + 0.008*\"source\"\n",
      "2022-05-03 17:19:08,931 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"work\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 17:19:08,932 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"big\" + 0.015*\"sort\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"unknowable\" + 0.015*\"question\" + 0.011*\"book\" + 0.010*\"distant\" + 0.010*\"reason\"\n",
      "2022-05-03 17:19:08,932 : INFO : topic diff=0.010166, rho=0.377964\n",
      "2022-05-03 17:19:08,962 : INFO : -6.662 per-word bound, 101.3 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:08,962 : INFO : PROGRESS: pass 6, at document #175/175\n",
      "2022-05-03 17:19:08,981 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"olé\" + 0.025*\"genius\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.017*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:08,981 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.018*\"afraid\" + 0.018*\"century\" + 0.015*\"applause\" + 0.010*\"way\" + 0.010*\"laughter\" + 0.010*\"work\" + 0.010*\"dancer\" + 0.010*\"kind\"\n",
      "2022-05-03 17:19:08,982 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.011*\"piece\" + 0.008*\"source\"\n",
      "2022-05-03 17:19:08,983 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"work\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:08,983 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"big\" + 0.015*\"human\" + 0.015*\"sort\" + 0.015*\"divine\" + 0.015*\"unknowable\" + 0.015*\"question\" + 0.011*\"book\" + 0.010*\"idea\" + 0.010*\"distant\"\n",
      "2022-05-03 17:19:08,984 : INFO : topic diff=0.006880, rho=0.353553\n",
      "2022-05-03 17:19:09,013 : INFO : -6.661 per-word bound, 101.2 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:09,014 : INFO : PROGRESS: pass 7, at document #175/175\n",
      "2022-05-03 17:19:09,033 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"olé\" + 0.025*\"genius\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:09,033 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.018*\"afraid\" + 0.018*\"century\" + 0.015*\"applause\" + 0.010*\"way\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"work\"\n",
      "2022-05-03 17:19:09,034 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.012*\"piece\" + 0.008*\"source\"\n",
      "2022-05-03 17:19:09,034 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"work\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:09,035 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"human\" + 0.015*\"big\" + 0.015*\"divine\" + 0.015*\"sort\" + 0.015*\"unknowable\" + 0.015*\"question\" + 0.011*\"book\" + 0.010*\"idea\" + 0.010*\"somebody\"\n",
      "2022-05-03 17:19:09,036 : INFO : topic diff=0.004727, rho=0.333333\n",
      "2022-05-03 17:19:09,066 : INFO : -6.661 per-word bound, 101.2 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:09,067 : INFO : PROGRESS: pass 8, at document #175/175\n",
      "2022-05-03 17:19:09,087 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"olé\" + 0.025*\"genius\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:09,087 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.018*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"way\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:09,088 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.012*\"piece\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:09,089 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"work\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:09,089 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"human\" + 0.015*\"big\" + 0.015*\"question\" + 0.015*\"divine\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"idea\" + 0.010*\"somebody\"\n",
      "2022-05-03 17:19:09,090 : INFO : topic diff=0.003305, rho=0.316228\n",
      "2022-05-03 17:19:09,119 : INFO : -6.661 per-word bound, 101.2 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:09,120 : INFO : PROGRESS: pass 9, at document #175/175\n",
      "2022-05-03 17:19:09,143 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"olé\" + 0.025*\"genius\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:09,144 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"way\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:09,144 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.012*\"piece\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:09,145 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"work\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:09,146 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"human\" + 0.015*\"question\" + 0.015*\"big\" + 0.015*\"divine\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:09,146 : INFO : topic diff=0.002356, rho=0.301511\n",
      "2022-05-03 17:19:09,175 : INFO : -6.661 per-word bound, 101.2 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:09,176 : INFO : PROGRESS: pass 10, at document #175/175\n",
      "2022-05-03 17:19:09,195 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"olé\" + 0.025*\"genius\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:09,196 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"way\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:09,196 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.012*\"piece\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:09,197 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"work\" + 0.016*\"thing\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:09,197 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"human\" + 0.015*\"question\" + 0.015*\"big\" + 0.015*\"divine\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:09,198 : INFO : topic diff=0.001716, rho=0.288675\n",
      "2022-05-03 17:19:09,228 : INFO : -6.660 per-word bound, 101.2 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:09,229 : INFO : PROGRESS: pass 11, at document #175/175\n",
      "2022-05-03 17:19:09,248 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:09,249 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"way\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:09,249 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.012*\"piece\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:09,250 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:09,251 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"human\" + 0.015*\"question\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 17:19:09,251 : INFO : topic diff=0.001282, rho=0.277350\n",
      "2022-05-03 17:19:09,279 : INFO : -6.660 per-word bound, 101.1 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:09,279 : INFO : PROGRESS: pass 12, at document #175/175\n",
      "2022-05-03 17:19:09,299 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:09,300 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"way\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:09,300 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.012*\"piece\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:09,301 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:09,302 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"human\" + 0.015*\"question\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:09,302 : INFO : topic diff=0.000995, rho=0.267261\n",
      "2022-05-03 17:19:09,332 : INFO : -6.660 per-word bound, 101.1 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:09,332 : INFO : PROGRESS: pass 13, at document #175/175\n",
      "2022-05-03 17:19:09,354 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:09,355 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"way\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:09,356 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.012*\"piece\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:09,356 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:09,357 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"human\" + 0.015*\"question\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:09,357 : INFO : topic diff=0.000844, rho=0.258199\n",
      "2022-05-03 17:19:09,387 : INFO : -6.660 per-word bound, 101.1 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:09,387 : INFO : PROGRESS: pass 14, at document #175/175\n",
      "2022-05-03 17:19:09,407 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:09,407 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"way\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:09,408 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.012*\"piece\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:09,408 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:09,409 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"human\" + 0.015*\"question\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:09,410 : INFO : topic diff=0.000899, rho=0.250000\n",
      "2022-05-03 17:19:09,439 : INFO : -6.659 per-word bound, 101.1 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:09,440 : INFO : PROGRESS: pass 15, at document #175/175\n",
      "2022-05-03 17:19:09,459 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:09,460 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"way\" + 0.010*\"kind\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:09,460 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.012*\"piece\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:09,461 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:09,462 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:09,462 : INFO : topic diff=0.001168, rho=0.242536\n",
      "2022-05-03 17:19:09,492 : INFO : -6.658 per-word bound, 101.0 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:09,492 : INFO : PROGRESS: pass 16, at document #175/175\n",
      "2022-05-03 17:19:09,513 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:09,514 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"way\" + 0.010*\"kind\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:09,514 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"allah\" + 0.012*\"piece\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:09,515 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:09,515 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:09,516 : INFO : topic diff=0.000818, rho=0.235702\n",
      "2022-05-03 17:19:09,546 : INFO : -6.658 per-word bound, 101.0 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:09,547 : INFO : PROGRESS: pass 17, at document #175/175\n",
      "2022-05-03 17:19:09,568 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:09,569 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"way\" + 0.010*\"kind\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:09,569 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:09,569 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:09,570 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:09,571 : INFO : topic diff=0.000628, rho=0.229416\n",
      "2022-05-03 17:19:09,601 : INFO : -6.658 per-word bound, 101.0 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 17:19:09,601 : INFO : PROGRESS: pass 18, at document #175/175\n",
      "2022-05-03 17:19:09,621 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:09,621 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"way\" + 0.010*\"kind\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:09,622 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:09,622 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:09,623 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:09,623 : INFO : topic diff=0.000503, rho=0.223607\n",
      "2022-05-03 17:19:09,655 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:09,656 : INFO : PROGRESS: pass 19, at document #175/175\n",
      "2022-05-03 17:19:09,675 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:09,676 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"way\" + 0.010*\"kind\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:09,676 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:09,677 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:09,678 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:09,678 : INFO : topic diff=0.000414, rho=0.218218\n",
      "2022-05-03 17:19:09,709 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:09,709 : INFO : PROGRESS: pass 20, at document #175/175\n",
      "2022-05-03 17:19:09,728 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:09,729 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:09,729 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:09,730 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:09,731 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:09,732 : INFO : topic diff=0.000346, rho=0.213201\n",
      "2022-05-03 17:19:09,762 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:09,763 : INFO : PROGRESS: pass 21, at document #175/175\n",
      "2022-05-03 17:19:09,781 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:09,782 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:09,783 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:09,783 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:09,784 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:09,785 : INFO : topic diff=0.000291, rho=0.208514\n",
      "2022-05-03 17:19:09,815 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:09,815 : INFO : PROGRESS: pass 22, at document #175/175\n",
      "2022-05-03 17:19:09,835 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:09,835 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:09,836 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:09,836 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:09,837 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:09,838 : INFO : topic diff=0.000246, rho=0.204124\n",
      "2022-05-03 17:19:09,868 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:09,868 : INFO : PROGRESS: pass 23, at document #175/175\n",
      "2022-05-03 17:19:09,887 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:09,888 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:09,888 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:09,889 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:09,889 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:09,890 : INFO : topic diff=0.000208, rho=0.200000\n",
      "2022-05-03 17:19:09,919 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:09,920 : INFO : PROGRESS: pass 24, at document #175/175\n",
      "2022-05-03 17:19:09,939 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 17:19:09,940 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:09,940 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:09,941 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:09,941 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:09,942 : INFO : topic diff=0.000175, rho=0.196116\n",
      "2022-05-03 17:19:09,972 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:09,972 : INFO : PROGRESS: pass 25, at document #175/175\n",
      "2022-05-03 17:19:09,992 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:09,993 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:09,993 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:09,993 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:09,994 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:09,995 : INFO : topic diff=0.000147, rho=0.192450\n",
      "2022-05-03 17:19:10,026 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:10,026 : INFO : PROGRESS: pass 26, at document #175/175\n",
      "2022-05-03 17:19:10,047 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:10,048 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:10,048 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:10,049 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:10,049 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:10,050 : INFO : topic diff=0.000124, rho=0.188982\n",
      "2022-05-03 17:19:10,080 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:10,081 : INFO : PROGRESS: pass 27, at document #175/175\n",
      "2022-05-03 17:19:10,100 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:10,101 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:10,101 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:10,101 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:10,102 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:10,102 : INFO : topic diff=0.000103, rho=0.185695\n",
      "2022-05-03 17:19:10,133 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:10,133 : INFO : PROGRESS: pass 28, at document #175/175\n",
      "2022-05-03 17:19:10,152 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:10,153 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:10,153 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:10,154 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:10,155 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:10,155 : INFO : topic diff=0.000086, rho=0.182574\n",
      "2022-05-03 17:19:10,186 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:10,186 : INFO : PROGRESS: pass 29, at document #175/175\n",
      "2022-05-03 17:19:10,206 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:10,206 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:10,207 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:10,208 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:10,208 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:10,209 : INFO : topic diff=0.000072, rho=0.179605\n",
      "2022-05-03 17:19:10,239 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:10,240 : INFO : PROGRESS: pass 30, at document #175/175\n",
      "2022-05-03 17:19:10,259 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:10,259 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"morning\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 17:19:10,260 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:10,260 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:10,261 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:10,262 : INFO : topic diff=0.000060, rho=0.176777\n",
      "2022-05-03 17:19:10,292 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:10,292 : INFO : PROGRESS: pass 31, at document #175/175\n",
      "2022-05-03 17:19:10,312 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:10,312 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:10,313 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:10,313 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:10,314 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:10,315 : INFO : topic diff=0.000050, rho=0.174078\n",
      "2022-05-03 17:19:10,344 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:10,344 : INFO : PROGRESS: pass 32, at document #175/175\n",
      "2022-05-03 17:19:10,364 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:10,364 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:10,365 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:10,365 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:10,366 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:10,366 : INFO : topic diff=0.000042, rho=0.171499\n",
      "2022-05-03 17:19:10,397 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:10,397 : INFO : PROGRESS: pass 33, at document #175/175\n",
      "2022-05-03 17:19:10,417 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:10,417 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:10,418 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:10,418 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:10,419 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:10,419 : INFO : topic diff=0.000035, rho=0.169031\n",
      "2022-05-03 17:19:10,455 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:10,455 : INFO : PROGRESS: pass 34, at document #175/175\n",
      "2022-05-03 17:19:10,475 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:10,475 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:10,476 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:10,476 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:10,477 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:10,478 : INFO : topic diff=0.000029, rho=0.166667\n",
      "2022-05-03 17:19:10,508 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:10,509 : INFO : PROGRESS: pass 35, at document #175/175\n",
      "2022-05-03 17:19:10,529 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:10,529 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:10,530 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:10,531 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:10,532 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:10,533 : INFO : topic diff=0.000024, rho=0.164399\n",
      "2022-05-03 17:19:10,564 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:10,564 : INFO : PROGRESS: pass 36, at document #175/175\n",
      "2022-05-03 17:19:10,583 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:10,584 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:10,584 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 17:19:10,585 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:10,586 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:10,586 : INFO : topic diff=0.000020, rho=0.162221\n",
      "2022-05-03 17:19:10,617 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:10,617 : INFO : PROGRESS: pass 37, at document #175/175\n",
      "2022-05-03 17:19:10,636 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:10,638 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:10,639 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:10,640 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:10,640 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:10,643 : INFO : topic diff=0.000017, rho=0.160128\n",
      "2022-05-03 17:19:10,677 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:10,678 : INFO : PROGRESS: pass 38, at document #175/175\n",
      "2022-05-03 17:19:10,697 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:10,698 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:10,698 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:10,699 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:10,700 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:10,700 : INFO : topic diff=0.000014, rho=0.158114\n",
      "2022-05-03 17:19:10,730 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:10,731 : INFO : PROGRESS: pass 39, at document #175/175\n",
      "2022-05-03 17:19:10,751 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:10,752 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:10,752 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:10,754 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:10,755 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:10,755 : INFO : topic diff=0.000012, rho=0.156174\n",
      "2022-05-03 17:19:10,786 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:10,787 : INFO : PROGRESS: pass 40, at document #175/175\n",
      "2022-05-03 17:19:10,806 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:10,807 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"morning\"\n",
      "2022-05-03 17:19:10,808 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:10,809 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:10,809 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:10,810 : INFO : topic diff=0.000010, rho=0.154303\n",
      "2022-05-03 17:19:10,840 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:10,840 : INFO : PROGRESS: pass 41, at document #175/175\n",
      "2022-05-03 17:19:10,860 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:10,860 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:10,861 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:10,862 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:10,862 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:10,863 : INFO : topic diff=0.000009, rho=0.152499\n",
      "2022-05-03 17:19:10,894 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:10,895 : INFO : PROGRESS: pass 42, at document #175/175\n",
      "2022-05-03 17:19:10,913 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:10,914 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:10,914 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:10,915 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 17:19:10,916 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:10,916 : INFO : topic diff=0.000007, rho=0.150756\n",
      "2022-05-03 17:19:10,946 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:10,947 : INFO : PROGRESS: pass 43, at document #175/175\n",
      "2022-05-03 17:19:10,966 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:10,967 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:10,967 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:10,968 : INFO : topic #3 (0.200): 0.019*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:10,969 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:10,969 : INFO : topic diff=0.000418, rho=0.149071\n",
      "2022-05-03 17:19:11,001 : INFO : -6.657 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:11,001 : INFO : PROGRESS: pass 44, at document #175/175\n",
      "2022-05-03 17:19:11,021 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:11,022 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:11,023 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:11,023 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:11,024 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:11,024 : INFO : topic diff=0.000361, rho=0.147442\n",
      "2022-05-03 17:19:11,054 : INFO : -6.656 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:11,055 : INFO : PROGRESS: pass 45, at document #175/175\n",
      "2022-05-03 17:19:11,074 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:11,074 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:11,075 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:11,076 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:11,076 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:11,076 : INFO : topic diff=0.000317, rho=0.145865\n",
      "2022-05-03 17:19:11,106 : INFO : -6.656 per-word bound, 100.9 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:11,107 : INFO : PROGRESS: pass 46, at document #175/175\n",
      "2022-05-03 17:19:11,125 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:11,126 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:11,126 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:11,126 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:11,127 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:11,127 : INFO : topic diff=0.000282, rho=0.144338\n",
      "2022-05-03 17:19:11,157 : INFO : -6.656 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:11,158 : INFO : PROGRESS: pass 47, at document #175/175\n",
      "2022-05-03 17:19:11,179 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:11,179 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:11,180 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:11,181 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:11,181 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:11,182 : INFO : topic diff=0.000254, rho=0.142857\n",
      "2022-05-03 17:19:11,212 : INFO : -6.656 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:11,213 : INFO : PROGRESS: pass 48, at document #175/175\n",
      "2022-05-03 17:19:11,231 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:11,232 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:11,233 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:11,233 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:11,234 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 17:19:11,234 : INFO : topic diff=0.000230, rho=0.141421\n",
      "2022-05-03 17:19:11,265 : INFO : -6.656 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:11,266 : INFO : PROGRESS: pass 49, at document #175/175\n",
      "2022-05-03 17:19:11,285 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:11,285 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:11,286 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:11,286 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:11,287 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:11,288 : INFO : topic diff=0.000211, rho=0.140028\n",
      "2022-05-03 17:19:11,319 : INFO : -6.656 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:11,320 : INFO : PROGRESS: pass 50, at document #175/175\n",
      "2022-05-03 17:19:11,338 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:11,339 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:11,339 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:11,340 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:11,341 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:11,341 : INFO : topic diff=0.000193, rho=0.138675\n",
      "2022-05-03 17:19:11,371 : INFO : -6.656 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:11,371 : INFO : PROGRESS: pass 51, at document #175/175\n",
      "2022-05-03 17:19:11,390 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:11,390 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:11,391 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:11,392 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:11,392 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:11,393 : INFO : topic diff=0.000179, rho=0.137361\n",
      "2022-05-03 17:19:11,423 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:11,423 : INFO : PROGRESS: pass 52, at document #175/175\n",
      "2022-05-03 17:19:11,442 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:11,443 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:11,443 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:11,444 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:11,444 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:11,445 : INFO : topic diff=0.000165, rho=0.136083\n",
      "2022-05-03 17:19:11,475 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:11,476 : INFO : PROGRESS: pass 53, at document #175/175\n",
      "2022-05-03 17:19:11,494 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:11,495 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:11,496 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:11,496 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:11,497 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:11,497 : INFO : topic diff=0.000154, rho=0.134840\n",
      "2022-05-03 17:19:11,527 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:11,527 : INFO : PROGRESS: pass 54, at document #175/175\n",
      "2022-05-03 17:19:11,546 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:11,547 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:11,548 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:11,548 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:11,549 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:11,549 : INFO : topic diff=0.000142, rho=0.133631\n",
      "2022-05-03 17:19:11,579 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 17:19:11,580 : INFO : PROGRESS: pass 55, at document #175/175\n",
      "2022-05-03 17:19:11,598 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:11,599 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:11,600 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:11,600 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:11,601 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:11,601 : INFO : topic diff=0.000132, rho=0.132453\n",
      "2022-05-03 17:19:11,633 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:11,633 : INFO : PROGRESS: pass 56, at document #175/175\n",
      "2022-05-03 17:19:11,652 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:11,653 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:11,653 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:11,654 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:11,655 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:11,656 : INFO : topic diff=0.000122, rho=0.131306\n",
      "2022-05-03 17:19:11,684 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:11,685 : INFO : PROGRESS: pass 57, at document #175/175\n",
      "2022-05-03 17:19:11,704 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:11,705 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:11,706 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:11,706 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:11,707 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:11,707 : INFO : topic diff=0.000113, rho=0.130189\n",
      "2022-05-03 17:19:11,737 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:11,738 : INFO : PROGRESS: pass 58, at document #175/175\n",
      "2022-05-03 17:19:11,757 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:11,757 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:11,758 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:11,758 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:11,759 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:11,759 : INFO : topic diff=0.000103, rho=0.129099\n",
      "2022-05-03 17:19:11,791 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:11,791 : INFO : PROGRESS: pass 59, at document #175/175\n",
      "2022-05-03 17:19:11,810 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:11,811 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:11,811 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:11,812 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:11,813 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:11,814 : INFO : topic diff=0.000095, rho=0.128037\n",
      "2022-05-03 17:19:11,844 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:11,845 : INFO : PROGRESS: pass 60, at document #175/175\n",
      "2022-05-03 17:19:11,863 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:11,864 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:11,865 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:11,865 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:11,865 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:11,866 : INFO : topic diff=0.000087, rho=0.127000\n",
      "2022-05-03 17:19:11,894 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:11,895 : INFO : PROGRESS: pass 61, at document #175/175\n",
      "2022-05-03 17:19:11,913 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 17:19:11,914 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:11,915 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:11,915 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:11,916 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:11,917 : INFO : topic diff=0.000079, rho=0.125988\n",
      "2022-05-03 17:19:11,947 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:11,948 : INFO : PROGRESS: pass 62, at document #175/175\n",
      "2022-05-03 17:19:11,967 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:11,968 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:11,968 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:11,969 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:11,970 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:11,970 : INFO : topic diff=0.000072, rho=0.125000\n",
      "2022-05-03 17:19:12,001 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:12,002 : INFO : PROGRESS: pass 63, at document #175/175\n",
      "2022-05-03 17:19:12,021 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:12,022 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:12,023 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:12,024 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:12,025 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:12,026 : INFO : topic diff=0.000065, rho=0.124035\n",
      "2022-05-03 17:19:12,057 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:12,058 : INFO : PROGRESS: pass 64, at document #175/175\n",
      "2022-05-03 17:19:12,078 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:12,079 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:12,079 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:12,080 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:12,081 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:12,081 : INFO : topic diff=0.000059, rho=0.123091\n",
      "2022-05-03 17:19:12,111 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:12,112 : INFO : PROGRESS: pass 65, at document #175/175\n",
      "2022-05-03 17:19:12,131 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:12,131 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:12,132 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:12,132 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:12,133 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:12,133 : INFO : topic diff=0.000053, rho=0.122169\n",
      "2022-05-03 17:19:12,164 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:12,164 : INFO : PROGRESS: pass 66, at document #175/175\n",
      "2022-05-03 17:19:12,183 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:12,184 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:12,185 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:12,185 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:12,186 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:12,187 : INFO : topic diff=0.000048, rho=0.121268\n",
      "2022-05-03 17:19:12,216 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:12,217 : INFO : PROGRESS: pass 67, at document #175/175\n",
      "2022-05-03 17:19:12,236 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:12,237 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 17:19:12,238 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:12,238 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:12,239 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:12,240 : INFO : topic diff=0.000043, rho=0.120386\n",
      "2022-05-03 17:19:12,270 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:12,270 : INFO : PROGRESS: pass 68, at document #175/175\n",
      "2022-05-03 17:19:12,289 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:12,290 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:12,290 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:12,291 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:12,292 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:12,292 : INFO : topic diff=0.000038, rho=0.119523\n",
      "2022-05-03 17:19:12,322 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:12,323 : INFO : PROGRESS: pass 69, at document #175/175\n",
      "2022-05-03 17:19:12,344 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:12,345 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:12,346 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:12,346 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:12,347 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:12,348 : INFO : topic diff=0.000034, rho=0.118678\n",
      "2022-05-03 17:19:12,378 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:12,379 : INFO : PROGRESS: pass 70, at document #175/175\n",
      "2022-05-03 17:19:12,399 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:12,399 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:12,400 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:12,401 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:12,401 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:12,402 : INFO : topic diff=0.000031, rho=0.117851\n",
      "2022-05-03 17:19:12,432 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:12,433 : INFO : PROGRESS: pass 71, at document #175/175\n",
      "2022-05-03 17:19:12,452 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:12,452 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:12,453 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:12,454 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:12,455 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:12,455 : INFO : topic diff=0.000027, rho=0.117041\n",
      "2022-05-03 17:19:12,484 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:12,485 : INFO : PROGRESS: pass 72, at document #175/175\n",
      "2022-05-03 17:19:12,504 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:12,505 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:12,506 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:12,506 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:12,507 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:12,508 : INFO : topic diff=0.000024, rho=0.116248\n",
      "2022-05-03 17:19:12,537 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:12,538 : INFO : PROGRESS: pass 73, at document #175/175\n",
      "2022-05-03 17:19:12,556 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:12,557 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:12,557 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 17:19:12,558 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:12,559 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:12,560 : INFO : topic diff=0.000022, rho=0.115470\n",
      "2022-05-03 17:19:12,589 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:12,589 : INFO : PROGRESS: pass 74, at document #175/175\n",
      "2022-05-03 17:19:12,608 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:12,609 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:12,609 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:12,610 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:12,611 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:12,611 : INFO : topic diff=0.000019, rho=0.114708\n",
      "2022-05-03 17:19:12,640 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:12,641 : INFO : PROGRESS: pass 75, at document #175/175\n",
      "2022-05-03 17:19:12,660 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:12,660 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:12,661 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:12,662 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:12,662 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:12,663 : INFO : topic diff=0.000017, rho=0.113961\n",
      "2022-05-03 17:19:12,692 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:12,693 : INFO : PROGRESS: pass 76, at document #175/175\n",
      "2022-05-03 17:19:12,711 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:12,712 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:12,712 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:12,713 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:12,714 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:12,714 : INFO : topic diff=0.000015, rho=0.113228\n",
      "2022-05-03 17:19:12,745 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:12,746 : INFO : PROGRESS: pass 77, at document #175/175\n",
      "2022-05-03 17:19:12,765 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:12,766 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:12,766 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:12,767 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:12,768 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:12,768 : INFO : topic diff=0.000014, rho=0.112509\n",
      "2022-05-03 17:19:12,799 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:12,799 : INFO : PROGRESS: pass 78, at document #175/175\n",
      "2022-05-03 17:19:12,818 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:12,819 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:12,820 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:12,820 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:12,821 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:12,821 : INFO : topic diff=0.000012, rho=0.111803\n",
      "2022-05-03 17:19:12,851 : INFO : -6.655 per-word bound, 100.8 perplexity estimate based on a held-out corpus of 175 documents with 825 words\n",
      "2022-05-03 17:19:12,851 : INFO : PROGRESS: pass 79, at document #175/175\n",
      "2022-05-03 17:19:12,872 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:12,873 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:12,874 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:12,875 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 17:19:12,876 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n",
      "2022-05-03 17:19:12,877 : INFO : topic diff=0.000011, rho=0.111111\n",
      "2022-05-03 17:19:12,878 : INFO : topic #0 (0.200): 0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"\n",
      "2022-05-03 17:19:12,879 : INFO : topic #1 (0.200): 0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"\n",
      "2022-05-03 17:19:12,880 : INFO : topic #2 (0.200): 0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"\n",
      "2022-05-03 17:19:12,881 : INFO : topic #3 (0.200): 0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"\n",
      "2022-05-03 17:19:12,882 : INFO : topic #4 (0.200): 0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.025*\"work\" + 0.025*\"genius\" + 0.025*\"olé\" + 0.018*\"sort\" + 0.018*\"artist\" + 0.018*\"laughter\" + 0.018*\"process\" + 0.014*\"creative\" + 0.011*\"year\" + 0.011*\"great\"'),\n",
       " (1,\n",
       "  '0.038*\"nt\" + 0.024*\"dance\" + 0.019*\"century\" + 0.018*\"afraid\" + 0.015*\"applause\" + 0.010*\"laughter\" + 0.010*\"dancer\" + 0.010*\"kind\" + 0.010*\"way\" + 0.010*\"hour\"'),\n",
       " (2,\n",
       "  '0.026*\"nt\" + 0.023*\"creative\" + 0.019*\"god\" + 0.019*\"way\" + 0.015*\"life\" + 0.015*\"idea\" + 0.012*\"piece\" + 0.012*\"allah\" + 0.012*\"sort\" + 0.008*\"year\"'),\n",
       " (3,\n",
       "  '0.018*\"book\" + 0.016*\"kind\" + 0.016*\"thing\" + 0.016*\"work\" + 0.016*\"poem\" + 0.012*\"creativity\" + 0.012*\"life\" + 0.012*\"ancient\" + 0.012*\"tom\" + 0.012*\"poet\"'),\n",
       " (4,\n",
       "  '0.029*\"thing\" + 0.015*\"question\" + 0.015*\"human\" + 0.015*\"divine\" + 0.015*\"big\" + 0.015*\"unknowable\" + 0.015*\"sort\" + 0.011*\"book\" + 0.010*\"somebody\" + 0.010*\"idea\"')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3bec4a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(0, 0.10000888),\n",
       "   (1, 0.10001142),\n",
       "   (2, 0.100009255),\n",
       "   (3, 0.100009516),\n",
       "   (4, 0.59996086)],\n",
       "  0),\n",
       " ([(0, 0.050009027),\n",
       "   (1, 0.050274476),\n",
       "   (2, 0.79747945),\n",
       "   (3, 0.051479295),\n",
       "   (4, 0.050757755)],\n",
       "  1),\n",
       " ([(0, 0.8392641),\n",
       "   (1, 0.040009554),\n",
       "   (2, 0.04040044),\n",
       "   (3, 0.04031631),\n",
       "   (4, 0.040009532)],\n",
       "  2),\n",
       " ([(0, 0.06667089),\n",
       "   (1, 0.06836166),\n",
       "   (2, 0.73162425),\n",
       "   (3, 0.06667119),\n",
       "   (4, 0.06667205)],\n",
       "  3),\n",
       " ([(0, 0.025208972),\n",
       "   (1, 0.025146488),\n",
       "   (2, 0.02526607),\n",
       "   (3, 0.89913976),\n",
       "   (4, 0.025238719)],\n",
       "  4),\n",
       " ([(0, 0.012516734),\n",
       "   (1, 0.01253968),\n",
       "   (2, 0.012534898),\n",
       "   (3, 0.012686873),\n",
       "   (4, 0.9497218)],\n",
       "  5),\n",
       " ([(0, 0.5999442),\n",
       "   (1, 0.10001532),\n",
       "   (2, 0.10001242),\n",
       "   (3, 0.10001277),\n",
       "   (4, 0.10001528)],\n",
       "  6),\n",
       " ([(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)], 7),\n",
       " ([(0, 0.04032088),\n",
       "   (1, 0.39868838),\n",
       "   (2, 0.0407268),\n",
       "   (3, 0.48025218),\n",
       "   (4, 0.04001175)],\n",
       "  8),\n",
       " ([(0, 0.022539578),\n",
       "   (1, 0.9089997),\n",
       "   (2, 0.022782726),\n",
       "   (3, 0.022996219),\n",
       "   (4, 0.022681782)],\n",
       "  9),\n",
       " ([(0, 0.10002528),\n",
       "   (1, 0.100032516),\n",
       "   (2, 0.10002634),\n",
       "   (3, 0.5998834),\n",
       "   (4, 0.10003243)],\n",
       "  10),\n",
       " ([(0, 0.03439123),\n",
       "   (1, 0.033520833),\n",
       "   (2, 0.03383792),\n",
       "   (3, 0.033431035),\n",
       "   (4, 0.86481893)],\n",
       "  11),\n",
       " ([(0, 0.050279927),\n",
       "   (1, 0.7975402),\n",
       "   (2, 0.05052866),\n",
       "   (3, 0.051160473),\n",
       "   (4, 0.050490767)],\n",
       "  12),\n",
       " ([(0, 0.033441618),\n",
       "   (1, 0.86630785),\n",
       "   (2, 0.0335359),\n",
       "   (3, 0.033376545),\n",
       "   (4, 0.03333805)],\n",
       "  13),\n",
       " ([(0, 0.014450829),\n",
       "   (1, 0.014460069),\n",
       "   (2, 0.9423639),\n",
       "   (3, 0.014392747),\n",
       "   (4, 0.014332441)],\n",
       "  14),\n",
       " ([(0, 0.59755385),\n",
       "   (1, 0.10185592),\n",
       "   (2, 0.100579895),\n",
       "   (3, 0.10000467),\n",
       "   (4, 0.100005664)],\n",
       "  15),\n",
       " ([(0, 0.03347558),\n",
       "   (1, 0.03333783),\n",
       "   (2, 0.033375416),\n",
       "   (3, 0.033376385),\n",
       "   (4, 0.86643475)],\n",
       "  16),\n",
       " ([(0, 0.79504144),\n",
       "   (1, 0.050893016),\n",
       "   (2, 0.050468788),\n",
       "   (3, 0.050963525),\n",
       "   (4, 0.05263319)],\n",
       "  17),\n",
       " ([(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)], 18),\n",
       " ([(0, 0.02912976),\n",
       "   (1, 0.029223375),\n",
       "   (2, 0.028857147),\n",
       "   (3, 0.8836618),\n",
       "   (4, 0.029127907)],\n",
       "  19),\n",
       " ([(0, 0.068550624),\n",
       "   (1, 0.066669814),\n",
       "   (2, 0.067596495),\n",
       "   (3, 0.06777845),\n",
       "   (4, 0.72940457)],\n",
       "  20),\n",
       " ([(0, 0.101520725),\n",
       "   (1, 0.1000142),\n",
       "   (2, 0.10001151),\n",
       "   (3, 0.5984372),\n",
       "   (4, 0.10001637)],\n",
       "  21),\n",
       " ([(0, 0.88493097),\n",
       "   (1, 0.028905576),\n",
       "   (2, 0.02869508),\n",
       "   (3, 0.028849421),\n",
       "   (4, 0.028618937)],\n",
       "  22),\n",
       " ([(0, 0.02008319),\n",
       "   (1, 0.020210732),\n",
       "   (2, 0.91928065),\n",
       "   (3, 0.020302841),\n",
       "   (4, 0.020122554)],\n",
       "  23),\n",
       " ([(0, 0.93813485),\n",
       "   (1, 0.015554172),\n",
       "   (2, 0.015467428),\n",
       "   (3, 0.015399192),\n",
       "   (4, 0.0154443085)],\n",
       "  24),\n",
       " ([(0, 0.066678695),\n",
       "   (1, 0.73328054),\n",
       "   (2, 0.06667919),\n",
       "   (3, 0.06667954),\n",
       "   (4, 0.06668203)],\n",
       "  25),\n",
       " ([(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)], 26),\n",
       " ([(0, 0.020289881),\n",
       "   (1, 0.919593),\n",
       "   (2, 0.020110153),\n",
       "   (3, 0.02000321),\n",
       "   (4, 0.020003816)],\n",
       "  27),\n",
       " ([(0, 0.025801353),\n",
       "   (1, 0.025596607),\n",
       "   (2, 0.8982917),\n",
       "   (3, 0.025236823),\n",
       "   (4, 0.025073512)],\n",
       "  28),\n",
       " ([(0, 0.020281514),\n",
       "   (1, 0.9189986),\n",
       "   (2, 0.020503413),\n",
       "   (3, 0.020147244),\n",
       "   (4, 0.020069288)],\n",
       "  29),\n",
       " ([(0, 0.033339225),\n",
       "   (1, 0.03361807),\n",
       "   (2, 0.8663622),\n",
       "   (3, 0.033339635),\n",
       "   (4, 0.03334083)],\n",
       "  30),\n",
       " ([(0, 0.033339113),\n",
       "   (1, 0.0336096),\n",
       "   (2, 0.033802312),\n",
       "   (3, 0.8658069),\n",
       "   (4, 0.03344209)],\n",
       "  31),\n",
       " ([(0, 0.041159574),\n",
       "   (1, 0.040592317),\n",
       "   (2, 0.041902676),\n",
       "   (3, 0.041980498),\n",
       "   (4, 0.8343649)],\n",
       "  32),\n",
       " ([(0, 0.01432696),\n",
       "   (1, 0.014425455),\n",
       "   (2, 0.0144381365),\n",
       "   (3, 0.9424201),\n",
       "   (4, 0.014389284)],\n",
       "  33),\n",
       " ([(0, 0.028723823),\n",
       "   (1, 0.028743088),\n",
       "   (2, 0.88424605),\n",
       "   (3, 0.028708206),\n",
       "   (4, 0.029578838)],\n",
       "  34),\n",
       " ([(0, 0.10001097),\n",
       "   (1, 0.1000141),\n",
       "   (2, 0.10001142),\n",
       "   (3, 0.5999494),\n",
       "   (4, 0.10001406)],\n",
       "  35),\n",
       " ([(0, 0.05000668),\n",
       "   (1, 0.050008535),\n",
       "   (2, 0.05000695),\n",
       "   (3, 0.7999693),\n",
       "   (4, 0.050008513)],\n",
       "  36),\n",
       " ([(0, 0.100026146),\n",
       "   (1, 0.100033626),\n",
       "   (2, 0.59987867),\n",
       "   (3, 0.10002802),\n",
       "   (4, 0.100033544)],\n",
       "  37),\n",
       " ([(0, 0.06702892),\n",
       "   (1, 0.4030924),\n",
       "   (2, 0.06668767),\n",
       "   (3, 0.39649853),\n",
       "   (4, 0.06669245)],\n",
       "  38),\n",
       " ([(0, 0.029306658),\n",
       "   (1, 0.02876578),\n",
       "   (2, 0.8847333),\n",
       "   (3, 0.028593166),\n",
       "   (4, 0.028601043)],\n",
       "  39),\n",
       " ([(0, 0.022493185),\n",
       "   (1, 0.022226373),\n",
       "   (2, 0.022371395),\n",
       "   (3, 0.9105372),\n",
       "   (4, 0.02237193)],\n",
       "  40),\n",
       " ([(0, 0.041156616),\n",
       "   (1, 0.8380195),\n",
       "   (2, 0.040531173),\n",
       "   (3, 0.040006254),\n",
       "   (4, 0.04028647)],\n",
       "  41),\n",
       " ([(0, 0.73172385),\n",
       "   (1, 0.06701877),\n",
       "   (2, 0.06694383),\n",
       "   (3, 0.06729294),\n",
       "   (4, 0.06702058)],\n",
       "  42),\n",
       " ([(0, 0.02516462),\n",
       "   (1, 0.025254576),\n",
       "   (2, 0.025081178),\n",
       "   (3, 0.89906806),\n",
       "   (4, 0.025431547)],\n",
       "  43),\n",
       " ([(0, 0.033579256),\n",
       "   (1, 0.033521842),\n",
       "   (2, 0.033481665),\n",
       "   (3, 0.034004223),\n",
       "   (4, 0.86541307)],\n",
       "  44),\n",
       " ([(0, 0.7316983),\n",
       "   (1, 0.06824675),\n",
       "   (2, 0.06668357),\n",
       "   (3, 0.066684045),\n",
       "   (4, 0.066687405)],\n",
       "  45),\n",
       " ([(0, 0.022361767),\n",
       "   (1, 0.91054565),\n",
       "   (2, 0.02233454),\n",
       "   (3, 0.022426406),\n",
       "   (4, 0.022331627)],\n",
       "  46),\n",
       " ([(0, 0.7310328),\n",
       "   (1, 0.06756396),\n",
       "   (2, 0.06710808),\n",
       "   (3, 0.06728319),\n",
       "   (4, 0.067012)],\n",
       "  47),\n",
       " ([(0, 0.10000563),\n",
       "   (1, 0.100007236),\n",
       "   (2, 0.1007708),\n",
       "   (3, 0.10078835),\n",
       "   (4, 0.598428)],\n",
       "  48),\n",
       " ([(0, 0.020513594),\n",
       "   (1, 0.020185366),\n",
       "   (2, 0.9189663),\n",
       "   (3, 0.020177614),\n",
       "   (4, 0.020157075)],\n",
       "  49),\n",
       " ([(0, 0.899256),\n",
       "   (1, 0.025134332),\n",
       "   (2, 0.025357151),\n",
       "   (3, 0.025041161),\n",
       "   (4, 0.025211355)],\n",
       "  50),\n",
       " ([(0, 0.015596106),\n",
       "   (1, 0.015454725),\n",
       "   (2, 0.9378811),\n",
       "   (3, 0.015546033),\n",
       "   (4, 0.015521984)],\n",
       "  51),\n",
       " ([(0, 0.8651878),\n",
       "   (1, 0.03334077),\n",
       "   (2, 0.0333394),\n",
       "   (3, 0.03457779),\n",
       "   (4, 0.033554215)],\n",
       "  52),\n",
       " ([(0, 0.06668207),\n",
       "   (1, 0.0666864),\n",
       "   (2, 0.733262),\n",
       "   (3, 0.06668315),\n",
       "   (4, 0.06668635)],\n",
       "  53),\n",
       " ([(0, 0.02534288),\n",
       "   (1, 0.025156232),\n",
       "   (2, 0.025053201),\n",
       "   (3, 0.89922196),\n",
       "   (4, 0.025225716)],\n",
       "  54),\n",
       " ([(0, 0.016835332),\n",
       "   (1, 0.016707763),\n",
       "   (2, 0.016739128),\n",
       "   (3, 0.016822988),\n",
       "   (4, 0.93289477)],\n",
       "  55),\n",
       " ([(0, 0.8840263),\n",
       "   (1, 0.028751204),\n",
       "   (2, 0.028854595),\n",
       "   (3, 0.029128455),\n",
       "   (4, 0.029239442)],\n",
       "  56),\n",
       " ([(0, 0.03351465),\n",
       "   (1, 0.033342473),\n",
       "   (2, 0.8664594),\n",
       "   (3, 0.033340994),\n",
       "   (4, 0.03334245)],\n",
       "  57),\n",
       " ([(0, 0.8836491),\n",
       "   (1, 0.028609661),\n",
       "   (2, 0.029778449),\n",
       "   (3, 0.028653055),\n",
       "   (4, 0.02930972)],\n",
       "  58),\n",
       " ([(0, 0.86606574),\n",
       "   (1, 0.033338755),\n",
       "   (2, 0.033389524),\n",
       "   (3, 0.03353689),\n",
       "   (4, 0.033669073)],\n",
       "  59),\n",
       " ([(0, 0.95510846),\n",
       "   (1, 0.011178196),\n",
       "   (2, 0.011203644),\n",
       "   (3, 0.011297674),\n",
       "   (4, 0.011212026)],\n",
       "  60),\n",
       " ([(0, 0.89937973),\n",
       "   (1, 0.02503727),\n",
       "   (2, 0.025353566),\n",
       "   (3, 0.02506132),\n",
       "   (4, 0.025168125)],\n",
       "  61),\n",
       " ([(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)], 62),\n",
       " ([(0, 0.029770976),\n",
       "   (1, 0.028576646),\n",
       "   (2, 0.028689317),\n",
       "   (3, 0.02905424),\n",
       "   (4, 0.8839088)],\n",
       "  63),\n",
       " ([(0, 0.029615235),\n",
       "   (1, 0.028704315),\n",
       "   (2, 0.028888553),\n",
       "   (3, 0.028805315),\n",
       "   (4, 0.8839866)],\n",
       "  64),\n",
       " ([(0, 0.07075316),\n",
       "   (1, 0.72441226),\n",
       "   (2, 0.06843799),\n",
       "   (3, 0.068662815),\n",
       "   (4, 0.06773375)],\n",
       "  65),\n",
       " ([(0, 0.052166432),\n",
       "   (1, 0.05124348),\n",
       "   (2, 0.05094078),\n",
       "   (3, 0.052425258),\n",
       "   (4, 0.79322404)],\n",
       "  66),\n",
       " ([(0, 0.040842574),\n",
       "   (1, 0.8372916),\n",
       "   (2, 0.040321738),\n",
       "   (3, 0.0412556),\n",
       "   (4, 0.040288486)],\n",
       "  67),\n",
       " ([(0, 0.011806885),\n",
       "   (1, 0.01178539),\n",
       "   (2, 0.011943394),\n",
       "   (3, 0.011835658),\n",
       "   (4, 0.9526286)],\n",
       "  68),\n",
       " ([(0, 0.033644088),\n",
       "   (1, 0.033400543),\n",
       "   (2, 0.033386283),\n",
       "   (3, 0.8659303),\n",
       "   (4, 0.03363875)],\n",
       "  69),\n",
       " ([(0, 0.8662879),\n",
       "   (1, 0.033336822),\n",
       "   (2, 0.03333618),\n",
       "   (3, 0.033537295),\n",
       "   (4, 0.033501763)],\n",
       "  70),\n",
       " ([(0, 0.06667874),\n",
       "   (1, 0.06668212),\n",
       "   (2, 0.06667923),\n",
       "   (3, 0.06667959),\n",
       "   (4, 0.7332803)],\n",
       "  71),\n",
       " ([(0, 0.01118077),\n",
       "   (1, 0.011152247),\n",
       "   (2, 0.011231553),\n",
       "   (3, 0.011150313),\n",
       "   (4, 0.9552851)],\n",
       "  72),\n",
       " ([(0, 0.050007045),\n",
       "   (1, 0.050212104),\n",
       "   (2, 0.05016687),\n",
       "   (3, 0.050173506),\n",
       "   (4, 0.7994405)],\n",
       "  73),\n",
       " ([(0, 0.033338953),\n",
       "   (1, 0.0333405),\n",
       "   (2, 0.03333918),\n",
       "   (3, 0.033494037),\n",
       "   (4, 0.8664873)],\n",
       "  74),\n",
       " ([(0, 0.7992059),\n",
       "   (1, 0.05017979),\n",
       "   (2, 0.050325714),\n",
       "   (3, 0.05000595),\n",
       "   (4, 0.05028261)],\n",
       "  75),\n",
       " ([(0, 0.05000331),\n",
       "   (1, 0.05000423),\n",
       "   (2, 0.05010383),\n",
       "   (3, 0.050106224),\n",
       "   (4, 0.7997824)],\n",
       "  76),\n",
       " ([(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)], 77),\n",
       " ([(0, 0.029301874),\n",
       "   (1, 0.028736548),\n",
       "   (2, 0.029778158),\n",
       "   (3, 0.8827284),\n",
       "   (4, 0.029455027)],\n",
       "  78),\n",
       " ([(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)], 79),\n",
       " ([(0, 0.8851741),\n",
       "   (1, 0.028625188),\n",
       "   (2, 0.028662758),\n",
       "   (3, 0.02891263),\n",
       "   (4, 0.028625356)],\n",
       "  80),\n",
       " ([(0, 0.016669802),\n",
       "   (1, 0.01667066),\n",
       "   (2, 0.93314207),\n",
       "   (3, 0.01679696),\n",
       "   (4, 0.01672056)],\n",
       "  81),\n",
       " ([(0, 0.10002044),\n",
       "   (1, 0.5999102),\n",
       "   (2, 0.10002129),\n",
       "   (3, 0.10002189),\n",
       "   (4, 0.100026205)],\n",
       "  82),\n",
       " ([(0, 0.05009456),\n",
       "   (1, 0.0502855),\n",
       "   (2, 0.05071545),\n",
       "   (3, 0.79584646),\n",
       "   (4, 0.053058)],\n",
       "  83),\n",
       " ([(0, 0.10125587),\n",
       "   (1, 0.1017517),\n",
       "   (2, 0.59698266),\n",
       "   (3, 0.10000446),\n",
       "   (4, 0.10000533)],\n",
       "  84),\n",
       " ([(0, 0.02957326),\n",
       "   (1, 0.02859912),\n",
       "   (2, 0.88445073),\n",
       "   (3, 0.028593792),\n",
       "   (4, 0.028783096)],\n",
       "  85),\n",
       " ([(0, 0.7996097),\n",
       "   (1, 0.050183285),\n",
       "   (2, 0.050084233),\n",
       "   (3, 0.050005957),\n",
       "   (4, 0.050116766)],\n",
       "  86),\n",
       " ([(0, 0.73327917),\n",
       "   (1, 0.06668151),\n",
       "   (2, 0.06667873),\n",
       "   (3, 0.06667907),\n",
       "   (4, 0.066681474)],\n",
       "  87),\n",
       " ([(0, 0.013364286),\n",
       "   (1, 0.013357223),\n",
       "   (2, 0.013404859),\n",
       "   (3, 0.9464991),\n",
       "   (4, 0.013374528)],\n",
       "  88),\n",
       " ([(0, 0.7993927),\n",
       "   (1, 0.05001489),\n",
       "   (2, 0.050012123),\n",
       "   (3, 0.050565425),\n",
       "   (4, 0.05001485)],\n",
       "  89),\n",
       " ([(0, 0.06667875),\n",
       "   (1, 0.7310503),\n",
       "   (2, 0.06667925),\n",
       "   (3, 0.06890958),\n",
       "   (4, 0.06668209)],\n",
       "  90),\n",
       " ([(0, 0.06765529),\n",
       "   (1, 0.06668576),\n",
       "   (2, 0.06668219),\n",
       "   (3, 0.73229104),\n",
       "   (4, 0.066685714)],\n",
       "  91),\n",
       " ([(0, 0.04019044),\n",
       "   (1, 0.040005136),\n",
       "   (2, 0.040461518),\n",
       "   (3, 0.8387811),\n",
       "   (4, 0.04056178)],\n",
       "  92),\n",
       " ([(0, 0.020058813),\n",
       "   (1, 0.020157313),\n",
       "   (2, 0.919022),\n",
       "   (3, 0.020757988),\n",
       "   (4, 0.020003913)],\n",
       "  93),\n",
       " ([(0, 0.040112525),\n",
       "   (1, 0.04015036),\n",
       "   (2, 0.040247604),\n",
       "   (3, 0.8394859),\n",
       "   (4, 0.0400036)],\n",
       "  94),\n",
       " ([(0, 0.067686215),\n",
       "   (1, 0.06667929),\n",
       "   (2, 0.7308672),\n",
       "   (3, 0.06667722),\n",
       "   (4, 0.06809007)],\n",
       "  95),\n",
       " ([(0, 0.02504189),\n",
       "   (1, 0.025380688),\n",
       "   (2, 0.025503742),\n",
       "   (3, 0.8990684),\n",
       "   (4, 0.025005251)],\n",
       "  96),\n",
       " ([(0, 0.03333916),\n",
       "   (1, 0.033340767),\n",
       "   (2, 0.034089725),\n",
       "   (3, 0.86588955),\n",
       "   (4, 0.03334075)],\n",
       "  97),\n",
       " ([(0, 0.025004452),\n",
       "   (1, 0.025005672),\n",
       "   (2, 0.8994585),\n",
       "   (3, 0.025525723),\n",
       "   (4, 0.025005657)],\n",
       "  98),\n",
       " ([(0, 0.83896047),\n",
       "   (1, 0.040219646),\n",
       "   (2, 0.040600892),\n",
       "   (3, 0.040063936),\n",
       "   (4, 0.040155075)],\n",
       "  99),\n",
       " ([(0, 0.6099828),\n",
       "   (1, 0.040314946),\n",
       "   (2, 0.040803526),\n",
       "   (3, 0.04008816),\n",
       "   (4, 0.2688105)],\n",
       "  100),\n",
       " ([(0, 0.029093692),\n",
       "   (1, 0.8843036),\n",
       "   (2, 0.028976362),\n",
       "   (3, 0.028920703),\n",
       "   (4, 0.028705655)],\n",
       "  101),\n",
       " ([(0, 0.10128408),\n",
       "   (1, 0.10000346),\n",
       "   (2, 0.10034809),\n",
       "   (3, 0.10207309),\n",
       "   (4, 0.59629124)],\n",
       "  102),\n",
       " ([(0, 0.100020654),\n",
       "   (1, 0.59660417),\n",
       "   (2, 0.103326574),\n",
       "   (3, 0.10002214),\n",
       "   (4, 0.100026496)],\n",
       "  103),\n",
       " ([(0, 0.06685918),\n",
       "   (1, 0.06667295),\n",
       "   (2, 0.7309075),\n",
       "   (3, 0.066671915),\n",
       "   (4, 0.06888842)],\n",
       "  104),\n",
       " ([(0, 0.101280786),\n",
       "   (1, 0.10000346),\n",
       "   (2, 0.10034759),\n",
       "   (3, 0.10201831),\n",
       "   (4, 0.59634984)],\n",
       "  105),\n",
       " ([(0, 0.8653122),\n",
       "   (1, 0.033860546),\n",
       "   (2, 0.03414894),\n",
       "   (3, 0.033338655),\n",
       "   (4, 0.033339664)],\n",
       "  106),\n",
       " ([(0, 0.018404143),\n",
       "   (1, 0.01821251),\n",
       "   (2, 0.9266803),\n",
       "   (3, 0.018428523),\n",
       "   (4, 0.018274473)],\n",
       "  107),\n",
       " ([(0, 0.94211346),\n",
       "   (1, 0.014335317),\n",
       "   (2, 0.014632704),\n",
       "   (3, 0.014553683),\n",
       "   (4, 0.014364867)],\n",
       "  108),\n",
       " ([(0, 0.033340264),\n",
       "   (1, 0.033572253),\n",
       "   (2, 0.033340547),\n",
       "   (3, 0.8664048),\n",
       "   (4, 0.033342153)],\n",
       "  109),\n",
       " ([(0, 0.015411735),\n",
       "   (1, 0.93827146),\n",
       "   (2, 0.015488023),\n",
       "   (3, 0.015441166),\n",
       "   (4, 0.015387661)],\n",
       "  110),\n",
       " ([(0, 0.040006794),\n",
       "   (1, 0.83905417),\n",
       "   (2, 0.04045697),\n",
       "   (3, 0.04047338),\n",
       "   (4, 0.040008653)],\n",
       "  111),\n",
       " ([(0, 0.91948104),\n",
       "   (1, 0.020165866),\n",
       "   (2, 0.020019662),\n",
       "   (3, 0.020158371),\n",
       "   (4, 0.02017503)],\n",
       "  112),\n",
       " ([(0, 0.10002048),\n",
       "   (1, 0.10002635),\n",
       "   (2, 0.10002135),\n",
       "   (3, 0.10002196),\n",
       "   (4, 0.5999099)],\n",
       "  113),\n",
       " ([(0, 0.10002044),\n",
       "   (1, 0.5999102),\n",
       "   (2, 0.10002129),\n",
       "   (3, 0.10002189),\n",
       "   (4, 0.100026205)],\n",
       "  114),\n",
       " ([(0, 0.052623745),\n",
       "   (1, 0.05001083),\n",
       "   (2, 0.050759815),\n",
       "   (3, 0.05000906),\n",
       "   (4, 0.7965965)],\n",
       "  115),\n",
       " ([(0, 0.59987134),\n",
       "   (1, 0.10003534),\n",
       "   (2, 0.10002863),\n",
       "   (3, 0.10002944),\n",
       "   (4, 0.10003524)],\n",
       "  116),\n",
       " ([(0, 0.59753287),\n",
       "   (1, 0.1018765),\n",
       "   (2, 0.10058028),\n",
       "   (3, 0.10000469),\n",
       "   (4, 0.10000567)],\n",
       "  117),\n",
       " ([(0, 0.040214013),\n",
       "   (1, 0.040292494),\n",
       "   (2, 0.04022302),\n",
       "   (3, 0.040007286),\n",
       "   (4, 0.8392632)],\n",
       "  118),\n",
       " ([(0, 0.06745301),\n",
       "   (1, 0.72701734),\n",
       "   (2, 0.06843777),\n",
       "   (3, 0.06753063),\n",
       "   (4, 0.06956121)],\n",
       "  119),\n",
       " ([(0, 0.5999442),\n",
       "   (1, 0.10001533),\n",
       "   (2, 0.10001242),\n",
       "   (3, 0.10001277),\n",
       "   (4, 0.10001528)],\n",
       "  120),\n",
       " ([(0, 0.7313215),\n",
       "   (1, 0.06701027),\n",
       "   (2, 0.06711076),\n",
       "   (3, 0.067289226),\n",
       "   (4, 0.06726823)],\n",
       "  121),\n",
       " ([(0, 0.05197636),\n",
       "   (1, 0.05056138),\n",
       "   (2, 0.050438605),\n",
       "   (3, 0.79645914),\n",
       "   (4, 0.050564494)],\n",
       "  122),\n",
       " ([(0, 0.9106884),\n",
       "   (1, 0.022226011),\n",
       "   (2, 0.022258312),\n",
       "   (3, 0.022336416),\n",
       "   (4, 0.02249085)],\n",
       "  123),\n",
       " ([(0, 0.018249877),\n",
       "   (1, 0.018212417),\n",
       "   (2, 0.018240564),\n",
       "   (3, 0.9270074),\n",
       "   (4, 0.018289747)],\n",
       "  124),\n",
       " ([(0, 0.033435505),\n",
       "   (1, 0.033935532),\n",
       "   (2, 0.86565423),\n",
       "   (3, 0.033527493),\n",
       "   (4, 0.03344726)],\n",
       "  125),\n",
       " ([(0, 0.015538969),\n",
       "   (1, 0.01543642),\n",
       "   (2, 0.015485129),\n",
       "   (3, 0.93797666),\n",
       "   (4, 0.015562875)],\n",
       "  126),\n",
       " ([(0, 0.05000707),\n",
       "   (1, 0.050140243),\n",
       "   (2, 0.05024669),\n",
       "   (3, 0.47913477),\n",
       "   (4, 0.3704712)],\n",
       "  127),\n",
       " ([(0, 0.10002074),\n",
       "   (1, 0.10002666),\n",
       "   (2, 0.10336388),\n",
       "   (3, 0.10002222),\n",
       "   (4, 0.59656656)],\n",
       "  128),\n",
       " ([(0, 0.050587047),\n",
       "   (1, 0.05001039),\n",
       "   (2, 0.0501331),\n",
       "   (3, 0.7992591),\n",
       "   (4, 0.050010365)],\n",
       "  129),\n",
       " ([(0, 0.028577324),\n",
       "   (1, 0.028578943),\n",
       "   (2, 0.028577559),\n",
       "   (3, 0.8855267),\n",
       "   (4, 0.028739417)],\n",
       "  130),\n",
       " ([(0, 0.025459891),\n",
       "   (1, 0.025337985),\n",
       "   (2, 0.8966261),\n",
       "   (3, 0.025886294),\n",
       "   (4, 0.02668971)],\n",
       "  131),\n",
       " ([(0, 0.100002),\n",
       "   (1, 0.59718674),\n",
       "   (2, 0.10280652),\n",
       "   (3, 0.10000217),\n",
       "   (4, 0.10000256)],\n",
       "  132),\n",
       " ([(0, 0.040006243),\n",
       "   (1, 0.04000797),\n",
       "   (2, 0.8392437),\n",
       "   (3, 0.040734068),\n",
       "   (4, 0.04000795)],\n",
       "  133),\n",
       " ([(0, 0.10001101),\n",
       "   (1, 0.10001416),\n",
       "   (2, 0.10160361),\n",
       "   (3, 0.5983571),\n",
       "   (4, 0.10001412)],\n",
       "  134),\n",
       " ([(0, 0.59858),\n",
       "   (1, 0.101394236),\n",
       "   (2, 0.1000079),\n",
       "   (3, 0.10000812),\n",
       "   (4, 0.100009724)],\n",
       "  135),\n",
       " ([(0, 0.8391426),\n",
       "   (1, 0.040535428),\n",
       "   (2, 0.04030443),\n",
       "   (3, 0.040008),\n",
       "   (4, 0.040009532)],\n",
       "  136),\n",
       " ([(0, 0.012559172),\n",
       "   (1, 0.949894),\n",
       "   (2, 0.012508619),\n",
       "   (3, 0.012536453),\n",
       "   (4, 0.012501742)],\n",
       "  137),\n",
       " ([(0, 0.7981171),\n",
       "   (1, 0.051843323),\n",
       "   (2, 0.05001216),\n",
       "   (3, 0.0500125),\n",
       "   (4, 0.0500149)],\n",
       "  138),\n",
       " ([(0, 0.06668209),\n",
       "   (1, 0.06668643),\n",
       "   (2, 0.7321467),\n",
       "   (3, 0.067798436),\n",
       "   (4, 0.06668637)],\n",
       "  139),\n",
       " ([(0, 0.040073037),\n",
       "   (1, 0.040098123),\n",
       "   (2, 0.040519048),\n",
       "   (3, 0.83885044),\n",
       "   (4, 0.040459316)],\n",
       "  140),\n",
       " ([(0, 0.025063692),\n",
       "   (1, 0.89957446),\n",
       "   (2, 0.025145404),\n",
       "   (3, 0.025172926),\n",
       "   (4, 0.025043517)],\n",
       "  141),\n",
       " ([(0, 0.066676214),\n",
       "   (1, 0.068052456),\n",
       "   (2, 0.06693844),\n",
       "   (3, 0.7302901),\n",
       "   (4, 0.0680428)],\n",
       "  142),\n",
       " ([(0, 0.7999662),\n",
       "   (1, 0.05000927),\n",
       "   (2, 0.050007544),\n",
       "   (3, 0.050007757),\n",
       "   (4, 0.050009243)],\n",
       "  143),\n",
       " ([(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)], 144),\n",
       " ([(0, 0.022442747),\n",
       "   (1, 0.02236563),\n",
       "   (2, 0.91042715),\n",
       "   (3, 0.022495717),\n",
       "   (4, 0.02226881)],\n",
       "  145),\n",
       " ([(0, 0.10000416),\n",
       "   (1, 0.10071302),\n",
       "   (2, 0.5979993),\n",
       "   (3, 0.10057135),\n",
       "   (4, 0.100712225)],\n",
       "  146),\n",
       " ([(0, 0.9596412),\n",
       "   (1, 0.010239145),\n",
       "   (2, 0.010045048),\n",
       "   (3, 0.010072595),\n",
       "   (4, 0.0100019835)],\n",
       "  147),\n",
       " ([(0, 0.014661701),\n",
       "   (1, 0.014397814),\n",
       "   (2, 0.014609739),\n",
       "   (3, 0.9420067),\n",
       "   (4, 0.0143240355)],\n",
       "  148),\n",
       " ([(0, 0.7329944),\n",
       "   (1, 0.06667987),\n",
       "   (2, 0.06696815),\n",
       "   (3, 0.066677704),\n",
       "   (4, 0.06667984)],\n",
       "  149),\n",
       " ([(0, 0.018199561),\n",
       "   (1, 0.926817),\n",
       "   (2, 0.018488562),\n",
       "   (3, 0.018262224),\n",
       "   (4, 0.01823267)],\n",
       "  150),\n",
       " ([(0, 0.033338316),\n",
       "   (1, 0.03333969),\n",
       "   (2, 0.03333852),\n",
       "   (3, 0.033338703),\n",
       "   (4, 0.8666448)],\n",
       "  151),\n",
       " ([(0, 0.033371426),\n",
       "   (1, 0.033421163),\n",
       "   (2, 0.8662807),\n",
       "   (3, 0.033506285),\n",
       "   (4, 0.033420436)],\n",
       "  152),\n",
       " ([(0, 0.100025296),\n",
       "   (1, 0.10003253),\n",
       "   (2, 0.100026354),\n",
       "   (3, 0.59988344),\n",
       "   (4, 0.10003244)],\n",
       "  153),\n",
       " ([(0, 0.040226836),\n",
       "   (1, 0.04012265),\n",
       "   (2, 0.83926743),\n",
       "   (3, 0.040255543),\n",
       "   (4, 0.040127512)],\n",
       "  154),\n",
       " ([(0, 0.03333923),\n",
       "   (1, 0.033615194),\n",
       "   (2, 0.8657326),\n",
       "   (3, 0.033737138),\n",
       "   (4, 0.033575807)],\n",
       "  155),\n",
       " ([(0, 0.025022218),\n",
       "   (1, 0.025151586),\n",
       "   (2, 0.8991688),\n",
       "   (3, 0.025178643),\n",
       "   (4, 0.025478775)],\n",
       "  156),\n",
       " ([(0, 0.06701927),\n",
       "   (1, 0.06714803),\n",
       "   (2, 0.73248863),\n",
       "   (3, 0.06667156),\n",
       "   (4, 0.066672504)],\n",
       "  157),\n",
       " ([(0, 0.03333754),\n",
       "   (1, 0.033473507),\n",
       "   (2, 0.033592593),\n",
       "   (3, 0.8660555),\n",
       "   (4, 0.033540864)],\n",
       "  158),\n",
       " ([(0, 0.04097532),\n",
       "   (1, 0.04140454),\n",
       "   (2, 0.8369005),\n",
       "   (3, 0.04035209),\n",
       "   (4, 0.040367603)],\n",
       "  159),\n",
       " ([(0, 0.10000199),\n",
       "   (1, 0.597265),\n",
       "   (2, 0.10272828),\n",
       "   (3, 0.10000217),\n",
       "   (4, 0.10000256)],\n",
       "  160),\n",
       " ([(0, 0.5985868),\n",
       "   (1, 0.10138743),\n",
       "   (2, 0.1000079),\n",
       "   (3, 0.10000812),\n",
       "   (4, 0.100009724)],\n",
       "  161),\n",
       " ([(0, 0.100007206),\n",
       "   (1, 0.10000926),\n",
       "   (2, 0.5999666),\n",
       "   (3, 0.10000772),\n",
       "   (4, 0.10000923)],\n",
       "  162),\n",
       " ([(0, 0.051643584),\n",
       "   (1, 0.7983436),\n",
       "   (2, 0.050003953),\n",
       "   (3, 0.050004065),\n",
       "   (4, 0.050004844)],\n",
       "  163),\n",
       " ([(0, 0.8994365),\n",
       "   (1, 0.025004562),\n",
       "   (2, 0.025102982),\n",
       "   (3, 0.02513536),\n",
       "   (4, 0.025320603)],\n",
       "  164),\n",
       " ([(0, 0.1004095),\n",
       "   (1, 0.5995795),\n",
       "   (2, 0.10000338),\n",
       "   (3, 0.10000347),\n",
       "   (4, 0.10000416)],\n",
       "  165),\n",
       " ([(0, 0.59899026),\n",
       "   (1, 0.100003935),\n",
       "   (2, 0.10000319),\n",
       "   (3, 0.100998715),\n",
       "   (4, 0.10000393)],\n",
       "  166),\n",
       " ([(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)], 167),\n",
       " ([(0, 0.5999646),\n",
       "   (1, 0.100009724),\n",
       "   (2, 0.100007884),\n",
       "   (3, 0.10000811),\n",
       "   (4, 0.100009695)],\n",
       "  168),\n",
       " ([(0, 0.59892714),\n",
       "   (1, 0.10000394),\n",
       "   (2, 0.1000032),\n",
       "   (3, 0.10106176),\n",
       "   (4, 0.100003935)],\n",
       "  169),\n",
       " ([(0, 0.03351428),\n",
       "   (1, 0.0333425),\n",
       "   (2, 0.8644596),\n",
       "   (3, 0.03437412),\n",
       "   (4, 0.03430951)],\n",
       "  170),\n",
       " ([(0, 0.10000887),\n",
       "   (1, 0.59996104),\n",
       "   (2, 0.10000924),\n",
       "   (3, 0.1000095),\n",
       "   (4, 0.10001137)],\n",
       "  171),\n",
       " ([(0, 0.06687173),\n",
       "   (1, 0.73311377),\n",
       "   (2, 0.06667111),\n",
       "   (3, 0.06667124),\n",
       "   (4, 0.06667212)],\n",
       "  172),\n",
       " ([(0, 0.83818746),\n",
       "   (1, 0.041716006),\n",
       "   (2, 0.040006064),\n",
       "   (3, 0.04008306),\n",
       "   (4, 0.04000742)],\n",
       "  173),\n",
       " ([(0, 0.10072069),\n",
       "   (1, 0.5992602),\n",
       "   (2, 0.100005865),\n",
       "   (3, 0.10000603),\n",
       "   (4, 0.10000721)],\n",
       "  174)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for a in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "137cda88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Dom_Topic  Topic_Contri  \\\n",
      "0          4.0        0.6000   \n",
      "1          2.0        0.7975   \n",
      "2          0.0        0.8393   \n",
      "3          2.0        0.7316   \n",
      "4          3.0        0.8991   \n",
      "..         ...           ...   \n",
      "170        2.0        0.8645   \n",
      "171        1.0        0.6000   \n",
      "172        1.0        0.7331   \n",
      "173        0.0        0.8382   \n",
      "174        1.0        0.5992   \n",
      "\n",
      "                                                                        Keywords  \n",
      "0    thing, question, human, divine, big, unknowable, sort, book, somebody, idea  \n",
      "1                   nt, creative, god, way, life, idea, piece, allah, sort, year  \n",
      "2      work, genius, olé, sort, artist, laughter, process, creative, year, great  \n",
      "3                   nt, creative, god, way, life, idea, piece, allah, sort, year  \n",
      "4            book, kind, thing, work, poem, creativity, life, ancient, tom, poet  \n",
      "..                                                                           ...  \n",
      "170                 nt, creative, god, way, life, idea, piece, allah, sort, year  \n",
      "171      nt, dance, century, afraid, applause, laughter, dancer, kind, way, hour  \n",
      "172      nt, dance, century, afraid, applause, laughter, dancer, kind, way, hour  \n",
      "173    work, genius, olé, sort, artist, laughter, process, creative, year, great  \n",
      "174      nt, dance, century, afraid, applause, laughter, dancer, kind, way, hour  \n",
      "\n",
      "[175 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "sent_topics_df = pd.DataFrame()\n",
    "for i, row_list in enumerate(ldana[corpusna]):\n",
    "        row = row_list[0] if ldana.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldana.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "sent_topics_df.columns = ['Dom_Topic', 'Topic_Contri', 'Keywords']\n",
    "print(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cd582911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {}\n",
    "# sentences = \"\"\n",
    "# corpus = pd.read_pickle(\"corpus.pkl\")\n",
    "# corpus\n",
    "# i=0\n",
    "# a=0\n",
    "# z=0\n",
    "# x=0\n",
    "# while(a<len(sent_topics_df)-1):\n",
    "#     sentences = corpus.loc[a].at['transcript']\n",
    "#     if(sent_topics_df.loc[a].at[\"Dom_Topic\"] == sent_topics_df.loc[a+1].at[\"Dom_Topic\"]):\n",
    "#         while((a<len(sent_topics_df)-1) and (sent_topics_df.loc[a].at[\"Dom_Topic\"] == sent_topics_df.loc[a+1].at[\"Dom_Topic\"])):\n",
    "#             sentences += corpus.loc[a+1].at['transcript']\n",
    "#             a+=1\n",
    "#     data[i] = sentences\n",
    "#     i+=1\n",
    "#     a+=1\n",
    "# if(a<len(sent_topics_df)):\n",
    "#     data[i] = sentences = corpus.loc[a].at['transcript']\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4c84d54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '00:00 I writer .',\n",
       " 1: \"Writing book profession 's , course .\",\n",
       " 2: 'It also great lifelong love fascination .',\n",
       " 3: \"And I n't expect 's ever going change .\",\n",
       " 4: 'But , said , something kind peculiar happened recently life career , caused recalibrate whole relationship work .',\n",
       " 5: \"And peculiar thing I recently wrote book , memoir called `` Eat , Pray , Love '' , decidedly unlike previous book , went world reason , became big , mega-sensation , international bestseller thing .\",\n",
       " 6: \"The result everywhere I go , people treat like I 'm doomed .Seriously -- doomed , doomed !\",\n",
       " 7: \"Like , come , worried , say , `` Are n't afraid 're never going able top ?\",\n",
       " 8: \"Are n't afraid 're going keep writing whole life 're never going create book anybody world care , ever ? ''\",\n",
       " 9: \"01:07 So 's reassuring , know .\",\n",
       " 10: 'But would worse , except I happen remember 20 year ago , I teenager , I first started telling people I wanted writer , I met sort fear-based reaction .',\n",
       " 11: \"And people would say , `` Are n't afraid 're never going success ?Are n't afraid humiliation rejection kill ?\",\n",
       " 12: \"Are n't afraid 're going work whole life craft nothing 's ever going come 're going die scrap heap broken dream mouth filled bitter ash failure ? ''\",\n",
       " 13: '01:39 ( Laughter ) 01:40 Like , know .',\n",
       " 14: \"01:42 The answer -- short answer question , `` Yes . ''\",\n",
       " 15: \"Yes , I 'm afraid thing .And I always .\",\n",
       " 16: \"And I 'm afraid many , many thing besides people ca n't even guess , like seaweed thing scary .\",\n",
       " 17: \"But , come writing , thing I 've sort thinking lately , wondering lately , ?\",\n",
       " 18: 'You know , rational ?',\n",
       " 19: 'Is logical anybody expected afraid work feel put Earth .',\n",
       " 20: \"And specifically creative venture seems make u really nervous 's mental health way career kind n't , know ?\",\n",
       " 21: \"Like dad , example , chemical engineer I n't recall 40 year chemical engineering anybody asking afraid chemical engineer , know ?\",\n",
       " 22: \"`` That chemical-engineering block , John , 's going ? ''\",\n",
       " 23: \"It n't come like , know ?\",\n",
       " 24: \"But fair , chemical engineer group n't really earned reputation century alcoholic manic-depressive .\",\n",
       " 25: '02:53 ( Laughter ) 02:55 We writer , kind reputation , writer , creative people across genre , seems , reputation enormously mentally unstable .',\n",
       " 26: 'And look grim death count 20th century alone , really magnificent creative mind died young often hand , know ?',\n",
       " 27: \"And even one n't literally commit suicide seem really undone gift , know .\",\n",
       " 28: \"Norman Mailer , died , last interview , said , `` Every one book killed little . ''\",\n",
       " 29: \"An extraordinary statement make life 's work .\",\n",
       " 30: \"But n't even blink hear somebody say , 've heard kind stuff long somehow 've completely internalized accepted collectively notion creativity suffering somehow inherently linked artistry , end , always ultimately lead anguish .\",\n",
       " 31: '03:53 And question I want ask everybody today guy cool idea ?',\n",
       " 32: \"Are comfortable ?Because look even inch away , know -- I 'm comfortable assumption .\",\n",
       " 33: \"I think 's odious .\",\n",
       " 34: \"And I also think 's dangerous , I n't want see perpetuated next century .\",\n",
       " 35: \"I think 's better encourage great creative mind live .\",\n",
       " 36: \"04:17 And I definitely know , case -- situation -- would dangerous start sort leaking dark path assumption , particularly given circumstance I 'm right career .\",\n",
       " 37: \"Which -- know , like check , I 'm pretty young , I 'm 40 year old .\",\n",
       " 38: 'I still maybe another four decade work left .',\n",
       " 39: \"And 's exceedingly likely anything I write point forward going judged world work came freakish success last book , right ?\",\n",
       " 40: \"I put bluntly , 're sort friend -- 's exceedingly likely greatest success behind .\",\n",
       " 41: 'So Jesus , thought !',\n",
       " 42: \"That 's kind thought could lead person start drinking gin nine o'clock morning , I n't want go .\",\n",
       " 43: '05:11 ( Laughter ) 05:12 I would prefer keep work I love .',\n",
       " 44: '05:14 And , question becomes , ?',\n",
       " 45: 'And , seems , upon lot reflection , way I work , order continue writing , I create sort protective psychological construct , right ?',\n",
       " 46: 'I sort find way safe distance , I writing , natural anxiety reaction writing going , .',\n",
       " 47: \"And , I 've looking , last year , model , I 've sort looking across time , I 've trying find society see might better saner idea help creative people sort manage inherent emotional risk creativity .\",\n",
       " 48: '05:58 And search led ancient Greece ancient Rome .',\n",
       " 49: 'So stay , circle around back .',\n",
       " 50: 'But , ancient Greece ancient Rome -- people happen believe creativity came human being back , OK ?',\n",
       " 51: 'People believed creativity divine attendant spirit came human being distant unknowable source , distant unknowable reason .',\n",
       " 52: \"The Greeks famously called divine attendant spirit creativity `` daemon . ''\",\n",
       " 53: 'Socrates , famously , believed daemon spoke wisdom afar .',\n",
       " 54: \"06:35 The Romans idea , called sort disembodied creative spirit genius .Which great , Romans actually think genius particularly clever individual .They believed genius , sort magical divine entity , believed literally live wall artist 's studio , kind like Dobby house elf , would come sort invisibly assist artist work would shape outcome work .07:05 So brilliant -- , right , distance I 'm talking -- psychological construct protect result work .And everyone knew functioned , right ?\",\n",
       " 55: \"So ancient artist protected certain thing , like , example , much narcissism , right ?If work brilliant , could n't take credit , everybody knew disembodied genius helped .\",\n",
       " 56: 'If work bombed , entirely fault , know ?',\n",
       " 57: 'Everyone knew genius kind lame .',\n",
       " 58: '07:34 ( Laughter ) 07:36 And people thought creativity West really long time .',\n",
       " 59: \"And Renaissance came everything changed , big idea , big idea , let 's put individual human center universe god mystery , 's room mystical creature take dictation divine .\",\n",
       " 60: \"And 's beginning rational humanism , people started believe creativity came completely self individual .\",\n",
       " 61: 'And first time history , start hear people referring artist genius , rather genius .',\n",
       " 62: \"08:10 And I got tell , I think huge error .You know , I think allowing somebody , one mere person believe like , vessel , know , like font essence source divine , creative , unknowable , eternal mystery smidge much responsibility put one fragile , human psyche .It 's like asking somebody swallow sun .It completely warp distorts ego , creates unmanageable expectation performance .\",\n",
       " 63: 'And I think pressure killing artist last 500 year .',\n",
       " 64: '08:47 And , true , I think true , question becomes , ?',\n",
       " 65: 'Can differently ?',\n",
       " 66: 'Maybe go back ancient understanding relationship human creative mystery .',\n",
       " 67: \"Maybe .Maybe ca n't erase 500 year rational humanistic thought one 18 minute speech .\",\n",
       " 68: \"And 's probably people audience would raise really legitimate scientific suspicion notion , basically , fairy follow people around rubbing fairy juice project stuff .\",\n",
       " 69: \"I 'm , probably , going bring along .\",\n",
       " 70: '09:30 But question I kind want pose -- know , ?',\n",
       " 71: 'Why think way ?Because make much sense anything else I ever heard term explaining utter maddening capriciousness creative process .',\n",
       " 72: 'A process , anybody ever tried make something -- say basically everyone -- - know always behave rationally .And , fact , sometimes feel downright paranormal .',\n",
       " 73: \"09:59 I encounter recently I met extraordinary American poet Ruth Stone , 's 90 , 's poet entire life told growing rural Virginia , would working field , said would feel hear poem coming landscape .\",\n",
       " 74: 'And said like thunderous train air .',\n",
       " 75: 'And would come barreling landscape .',\n",
       " 76: \"And felt coming , would shake earth foot .She knew one thing point , , word , `` run like hell . ''\",\n",
       " 77: 'And would run like hell house would getting chased poem , whole deal get piece paper pencil fast enough thundered , could collect grab page .',\n",
       " 78: \"And time would n't fast enough , 'd running running , would n't get house poem would barrel would miss said would continue across landscape , looking , put `` another poet . ''\",\n",
       " 79: 'And time -- piece I never forgot -- said moment would almost miss , right ?',\n",
       " 80: \"So , 's running house 's looking paper poem pass , grab pencil 's going , said , like would reach hand would catch .She would catch poem tail , would pull backwards body transcribing page .\",\n",
       " 81: 'And instance , poem would come page perfect intact backwards , last word first .',\n",
       " 82: \"11:31 ( Laughter ) 11:33 So I heard I like -- 's uncanny , 's exactly creative process like .11:40 ( Laughter ) 11:44 That 's creative process -- I 'm pipeline !\",\n",
       " 83: \"I 'm mule , way I work I get time every day , sweat labor barrel really awkwardly .\",\n",
       " 84: 'But even I , mulishness , even I brushed thing , time .',\n",
       " 85: 'And I would imagine lot .',\n",
       " 86: 'You know , even I work idea come source I honestly identify .',\n",
       " 87: 'And thing ?',\n",
       " 88: 'And relate way make u lose mind , , fact , might actually keep u sane ?',\n",
       " 89: '12:17 And , best contemporary example I musician Tom Waits , I got interview several year ago magazine assignment .',\n",
       " 90: 'And talking , know , Tom , life , pretty much embodiment tormented contemporary modern artist , trying control manage dominate sort uncontrollable creative impulse totally internalized .',\n",
       " 91: '12:41 But got older , got calmer , one day driving freeway Los Angeles , changed .',\n",
       " 92: \"And 's speeding along , sudden hears little fragment melody , come head inspiration often come , elusive tantalizing , want , 's gorgeous , longs , way get .He n't piece paper , pencil , tape recorder .\",\n",
       " 93: \"13:05 So start feel old anxiety start rise like , `` I 'm going lose thing , I 'll haunted song forever .\",\n",
       " 94: \"I 'm good enough , I ca n't . ''\",\n",
       " 95: 'And instead panicking , stopped .',\n",
       " 96: 'He stopped whole mental process something completely novel .',\n",
       " 97: \"He looked sky , said , `` Excuse , see I 'm driving ? ''13:26 ( Laughter ) 13:30 '' Do I look like I write song right ?\",\n",
       " 98: 'If really want exist , come back opportune moment I take care .',\n",
       " 99: 'Otherwise , go bother somebody else today .',\n",
       " 100: \"Go bother Leonard Cohen . ''13:44 And whole work process changed .\",\n",
       " 101: 'Not work , work still oftentimes dark ever .',\n",
       " 102: \"But process , heavy anxiety around released took genie , genius causing nothing trouble , released back came , realized n't internalized , tormented thing .\",\n",
       " 103: 'It could peculiar , wondrous , bizarre collaboration , kind conversation Tom strange , external thing quite Tom .',\n",
       " 104: '14:14 When I heard story , started shift little bit way I worked , idea already saved .',\n",
       " 105: \"It saved I middle writing `` Eat , Pray , Love , '' I fell one sort pit despair fall 're working something 's coming start think going disaster , worst book ever written .Not bad , worst book ever written .\",\n",
       " 106: 'And I started think I dump project .',\n",
       " 107: 'But I remembered Tom talking open air I tried .So I lifted face manuscript I directed comment empty corner room .',\n",
       " 108: \"And I said aloud , `` Listen , thing , I know book n't brilliant entirely fault , right ?\",\n",
       " 109: \"Because see I putting everything I , I n't .\",\n",
       " 110: \"If want better , 've got show part deal .\",\n",
       " 111: \"But n't , know , hell .\",\n",
       " 112: \"I 'm going keep writing anyway 's job .And I would please like record reflect today I showed part job . ''\",\n",
       " 113: \"15:17 ( Laughter ) 15:20 Because -- 15:22 ( Applause ) 15:24 Because end 's like , OK -- century ago desert North Africa , people used gather moonlight dance sacred dance music would go hour hour , dawn .\",\n",
       " 114: 'They always magnificent , dancer professional terrific , right ?',\n",
       " 115: 'But every , rarely , something would happen , one performer would actually become transcendent .',\n",
       " 116: \"And I know know I 'm talking , I know 've seen , point life , performance like .\",\n",
       " 117: \"It like time would stop , dancer would sort step kind portal n't anything different ever done , 1,000 night , everything would align .\",\n",
       " 118: 'And sudden , would longer appear merely human .',\n",
       " 119: 'He would lit within , lit lit fire divinity .16:14 And happened , back , people knew , know , called name .',\n",
       " 120: \"They would put hand together would start chant , `` Allah , Allah , Allah , God , God , God . ''That 's God , know .\",\n",
       " 121: \"Curious historical footnote : Moors invaded southern Spain , took custom pronunciation changed century `` Allah , Allah , Allah , '' `` Olé , olé , olé , '' still hear bullfight flamenco dance .\",\n",
       " 122: \"In Spain , performer done something impossible magic , `` Allah , olé , olé , Allah , magnificent , bravo , '' incomprehensible , -- glimpse God .\",\n",
       " 123: 'Which great , need .',\n",
       " 124: \"16:58 But , tricky bit come next morning , dancer , wake discovers 's Tuesday 11 a.m. , 's longer glimpse God .\",\n",
       " 125: \"He 's aging mortal really bad knee , maybe 's never going ascend height .\",\n",
       " 126: \"And maybe nobody ever chant God 's name spin , rest life ?\",\n",
       " 127: 'This hard .',\n",
       " 128: \"This one painful reconciliation make creative life .But maybe n't quite full anguish never happened believe , first place , extraordinary aspect came .But maybe believed loan unimaginable source exquisite portion life passed along 're finished , somebody else .And , know , think way , start change everything .\",\n",
       " 129: \"17:55 This I 've started think , certainly I 've thinking last month I 've working book soon published , dangerously , frighteningly over-anticipated follow freakish success .\",\n",
       " 130: \"18:09 And I sort keep telling I get really psyched n't afraid .\",\n",
       " 131: \"Do n't daunted .\",\n",
       " 132: 'Just job .',\n",
       " 133: 'Continue show piece , whatever might .',\n",
       " 134: 'If job dance , dance .',\n",
       " 135: \"If divine , cockeyed genius assigned case decides let sort wonderment glimpsed , one moment effort , `` Olé ! ''\",\n",
       " 136: 'And , dance anyhow .',\n",
       " 137: \"And `` Olé ! '', nonetheless .I believe I feel must teach .`` Olé ! ''\",\n",
       " 138: ', nonetheless , sheer human love stubbornness keep showing .',\n",
       " 139: '18:51 Thank .18:53 ( Applause ) 18:55 Thank .',\n",
       " 140: '18:56 ( Applause ) 18:59 June Cohen : Olé !',\n",
       " 141: '19:01 ( Applause )'}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {}\n",
    "sentences = \"\"\n",
    "corpus = pd.read_pickle(\"corpus.pkl\")\n",
    "corpus\n",
    "i=0\n",
    "a=0\n",
    "z=0\n",
    "x=0\n",
    "y=1\n",
    "j=1\n",
    "time_and_sentences = pd.read_pickle('time_and_sentences.pkl')\n",
    "keys = list(time_and_sentences.keys())\n",
    "values = list(time_and_sentences.values())\n",
    "\n",
    "while(a<len(sent_topics_df)-1):\n",
    "    sentences = corpus.loc[a].at['transcript']\n",
    "    if(sent_topics_df.loc[a].at[\"Dom_Topic\"] == sent_topics_df.loc[a+1].at[\"Dom_Topic\"]):\n",
    "        while((a<len(sent_topics_df)-1) and (sent_topics_df.loc[a].at[\"Dom_Topic\"] == sent_topics_df.loc[a+1].at[\"Dom_Topic\"])):\n",
    "            sentences += corpus.loc[a+1].at['transcript']\n",
    "            if(y < len(values) and (values[y]-j) >= (a+1)):\n",
    "                values[y] = values[y]-j\n",
    "                y += 1\n",
    "                \n",
    "            else:\n",
    "                while(y < len(values) and (values[y]-j) < (a+1)):\n",
    "                    values[y] = values[y]-j\n",
    "                    y += 1\n",
    "                    \n",
    "            j += 1\n",
    "            a+=1\n",
    "    data[i] = sentences\n",
    "    i+=1\n",
    "    a+=1\n",
    "if(a<len(sent_topics_df)):\n",
    "    data[i] = sentences = corpus.loc[a].at['transcript']\n",
    "\n",
    "values.sort()\n",
    "\n",
    "time_and_sentences = dict(zip(keys, values))\n",
    "pickle.dump(time_and_sentences, open(\"time_and_sentences.pkl\", \"wb\" ))\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7dd1204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to change this to key: sentence_id, value: string format\n",
    "def combine_text(list_of_text):\n",
    "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ''.join(list_of_text)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5eb6005f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['00:00 I writer .'],\n",
       " 1: [\"Writing book profession 's , course .\"],\n",
       " 2: ['It also great lifelong love fascination .'],\n",
       " 3: [\"And I n't expect 's ever going change .\"],\n",
       " 4: ['But , said , something kind peculiar happened recently life career , caused recalibrate whole relationship work .'],\n",
       " 5: [\"And peculiar thing I recently wrote book , memoir called `` Eat , Pray , Love '' , decidedly unlike previous book , went world reason , became big , mega-sensation , international bestseller thing .\"],\n",
       " 6: [\"The result everywhere I go , people treat like I 'm doomed .Seriously -- doomed , doomed !\"],\n",
       " 7: [\"Like , come , worried , say , `` Are n't afraid 're never going able top ?\"],\n",
       " 8: [\"Are n't afraid 're going keep writing whole life 're never going create book anybody world care , ever ? ''\"],\n",
       " 9: [\"01:07 So 's reassuring , know .\"],\n",
       " 10: ['But would worse , except I happen remember 20 year ago , I teenager , I first started telling people I wanted writer , I met sort fear-based reaction .'],\n",
       " 11: [\"And people would say , `` Are n't afraid 're never going success ?Are n't afraid humiliation rejection kill ?\"],\n",
       " 12: [\"Are n't afraid 're going work whole life craft nothing 's ever going come 're going die scrap heap broken dream mouth filled bitter ash failure ? ''\"],\n",
       " 13: ['01:39 ( Laughter ) 01:40 Like , know .'],\n",
       " 14: [\"01:42 The answer -- short answer question , `` Yes . ''\"],\n",
       " 15: [\"Yes , I 'm afraid thing .And I always .\"],\n",
       " 16: [\"And I 'm afraid many , many thing besides people ca n't even guess , like seaweed thing scary .\"],\n",
       " 17: [\"But , come writing , thing I 've sort thinking lately , wondering lately , ?\"],\n",
       " 18: ['You know , rational ?'],\n",
       " 19: ['Is logical anybody expected afraid work feel put Earth .'],\n",
       " 20: [\"And specifically creative venture seems make u really nervous 's mental health way career kind n't , know ?\"],\n",
       " 21: [\"Like dad , example , chemical engineer I n't recall 40 year chemical engineering anybody asking afraid chemical engineer , know ?\"],\n",
       " 22: [\"`` That chemical-engineering block , John , 's going ? ''\"],\n",
       " 23: [\"It n't come like , know ?\"],\n",
       " 24: [\"But fair , chemical engineer group n't really earned reputation century alcoholic manic-depressive .\"],\n",
       " 25: ['02:53 ( Laughter ) 02:55 We writer , kind reputation , writer , creative people across genre , seems , reputation enormously mentally unstable .'],\n",
       " 26: ['And look grim death count 20th century alone , really magnificent creative mind died young often hand , know ?'],\n",
       " 27: [\"And even one n't literally commit suicide seem really undone gift , know .\"],\n",
       " 28: [\"Norman Mailer , died , last interview , said , `` Every one book killed little . ''\"],\n",
       " 29: [\"An extraordinary statement make life 's work .\"],\n",
       " 30: [\"But n't even blink hear somebody say , 've heard kind stuff long somehow 've completely internalized accepted collectively notion creativity suffering somehow inherently linked artistry , end , always ultimately lead anguish .\"],\n",
       " 31: ['03:53 And question I want ask everybody today guy cool idea ?'],\n",
       " 32: [\"Are comfortable ?Because look even inch away , know -- I 'm comfortable assumption .\"],\n",
       " 33: [\"I think 's odious .\"],\n",
       " 34: [\"And I also think 's dangerous , I n't want see perpetuated next century .\"],\n",
       " 35: [\"I think 's better encourage great creative mind live .\"],\n",
       " 36: [\"04:17 And I definitely know , case -- situation -- would dangerous start sort leaking dark path assumption , particularly given circumstance I 'm right career .\"],\n",
       " 37: [\"Which -- know , like check , I 'm pretty young , I 'm 40 year old .\"],\n",
       " 38: ['I still maybe another four decade work left .'],\n",
       " 39: [\"And 's exceedingly likely anything I write point forward going judged world work came freakish success last book , right ?\"],\n",
       " 40: [\"I put bluntly , 're sort friend -- 's exceedingly likely greatest success behind .\"],\n",
       " 41: ['So Jesus , thought !'],\n",
       " 42: [\"That 's kind thought could lead person start drinking gin nine o'clock morning , I n't want go .\"],\n",
       " 43: ['05:11 ( Laughter ) 05:12 I would prefer keep work I love .'],\n",
       " 44: ['05:14 And , question becomes , ?'],\n",
       " 45: ['And , seems , upon lot reflection , way I work , order continue writing , I create sort protective psychological construct , right ?'],\n",
       " 46: ['I sort find way safe distance , I writing , natural anxiety reaction writing going , .'],\n",
       " 47: [\"And , I 've looking , last year , model , I 've sort looking across time , I 've trying find society see might better saner idea help creative people sort manage inherent emotional risk creativity .\"],\n",
       " 48: ['05:58 And search led ancient Greece ancient Rome .'],\n",
       " 49: ['So stay , circle around back .'],\n",
       " 50: ['But , ancient Greece ancient Rome -- people happen believe creativity came human being back , OK ?'],\n",
       " 51: ['People believed creativity divine attendant spirit came human being distant unknowable source , distant unknowable reason .'],\n",
       " 52: [\"The Greeks famously called divine attendant spirit creativity `` daemon . ''\"],\n",
       " 53: ['Socrates , famously , believed daemon spoke wisdom afar .'],\n",
       " 54: [\"06:35 The Romans idea , called sort disembodied creative spirit genius .Which great , Romans actually think genius particularly clever individual .They believed genius , sort magical divine entity , believed literally live wall artist 's studio , kind like Dobby house elf , would come sort invisibly assist artist work would shape outcome work .07:05 So brilliant -- , right , distance I 'm talking -- psychological construct protect result work .And everyone knew functioned , right ?\"],\n",
       " 55: [\"So ancient artist protected certain thing , like , example , much narcissism , right ?If work brilliant , could n't take credit , everybody knew disembodied genius helped .\"],\n",
       " 56: ['If work bombed , entirely fault , know ?'],\n",
       " 57: ['Everyone knew genius kind lame .'],\n",
       " 58: ['07:34 ( Laughter ) 07:36 And people thought creativity West really long time .'],\n",
       " 59: [\"And Renaissance came everything changed , big idea , big idea , let 's put individual human center universe god mystery , 's room mystical creature take dictation divine .\"],\n",
       " 60: [\"And 's beginning rational humanism , people started believe creativity came completely self individual .\"],\n",
       " 61: ['And first time history , start hear people referring artist genius , rather genius .'],\n",
       " 62: [\"08:10 And I got tell , I think huge error .You know , I think allowing somebody , one mere person believe like , vessel , know , like font essence source divine , creative , unknowable , eternal mystery smidge much responsibility put one fragile , human psyche .It 's like asking somebody swallow sun .It completely warp distorts ego , creates unmanageable expectation performance .\"],\n",
       " 63: ['And I think pressure killing artist last 500 year .'],\n",
       " 64: ['08:47 And , true , I think true , question becomes , ?'],\n",
       " 65: ['Can differently ?'],\n",
       " 66: ['Maybe go back ancient understanding relationship human creative mystery .'],\n",
       " 67: [\"Maybe .Maybe ca n't erase 500 year rational humanistic thought one 18 minute speech .\"],\n",
       " 68: [\"And 's probably people audience would raise really legitimate scientific suspicion notion , basically , fairy follow people around rubbing fairy juice project stuff .\"],\n",
       " 69: [\"I 'm , probably , going bring along .\"],\n",
       " 70: ['09:30 But question I kind want pose -- know , ?'],\n",
       " 71: ['Why think way ?Because make much sense anything else I ever heard term explaining utter maddening capriciousness creative process .'],\n",
       " 72: ['A process , anybody ever tried make something -- say basically everyone -- - know always behave rationally .And , fact , sometimes feel downright paranormal .'],\n",
       " 73: [\"09:59 I encounter recently I met extraordinary American poet Ruth Stone , 's 90 , 's poet entire life told growing rural Virginia , would working field , said would feel hear poem coming landscape .\"],\n",
       " 74: ['And said like thunderous train air .'],\n",
       " 75: ['And would come barreling landscape .'],\n",
       " 76: [\"And felt coming , would shake earth foot .She knew one thing point , , word , `` run like hell . ''\"],\n",
       " 77: ['And would run like hell house would getting chased poem , whole deal get piece paper pencil fast enough thundered , could collect grab page .'],\n",
       " 78: [\"And time would n't fast enough , 'd running running , would n't get house poem would barrel would miss said would continue across landscape , looking , put `` another poet . ''\"],\n",
       " 79: ['And time -- piece I never forgot -- said moment would almost miss , right ?'],\n",
       " 80: [\"So , 's running house 's looking paper poem pass , grab pencil 's going , said , like would reach hand would catch .She would catch poem tail , would pull backwards body transcribing page .\"],\n",
       " 81: ['And instance , poem would come page perfect intact backwards , last word first .'],\n",
       " 82: [\"11:31 ( Laughter ) 11:33 So I heard I like -- 's uncanny , 's exactly creative process like .11:40 ( Laughter ) 11:44 That 's creative process -- I 'm pipeline !\"],\n",
       " 83: [\"I 'm mule , way I work I get time every day , sweat labor barrel really awkwardly .\"],\n",
       " 84: ['But even I , mulishness , even I brushed thing , time .'],\n",
       " 85: ['And I would imagine lot .'],\n",
       " 86: ['You know , even I work idea come source I honestly identify .'],\n",
       " 87: ['And thing ?'],\n",
       " 88: ['And relate way make u lose mind , , fact , might actually keep u sane ?'],\n",
       " 89: ['12:17 And , best contemporary example I musician Tom Waits , I got interview several year ago magazine assignment .'],\n",
       " 90: ['And talking , know , Tom , life , pretty much embodiment tormented contemporary modern artist , trying control manage dominate sort uncontrollable creative impulse totally internalized .'],\n",
       " 91: ['12:41 But got older , got calmer , one day driving freeway Los Angeles , changed .'],\n",
       " 92: [\"And 's speeding along , sudden hears little fragment melody , come head inspiration often come , elusive tantalizing , want , 's gorgeous , longs , way get .He n't piece paper , pencil , tape recorder .\"],\n",
       " 93: [\"13:05 So start feel old anxiety start rise like , `` I 'm going lose thing , I 'll haunted song forever .\"],\n",
       " 94: [\"I 'm good enough , I ca n't . ''\"],\n",
       " 95: ['And instead panicking , stopped .'],\n",
       " 96: ['He stopped whole mental process something completely novel .'],\n",
       " 97: [\"He looked sky , said , `` Excuse , see I 'm driving ? ''13:26 ( Laughter ) 13:30 '' Do I look like I write song right ?\"],\n",
       " 98: ['If really want exist , come back opportune moment I take care .'],\n",
       " 99: ['Otherwise , go bother somebody else today .'],\n",
       " 100: [\"Go bother Leonard Cohen . ''13:44 And whole work process changed .\"],\n",
       " 101: ['Not work , work still oftentimes dark ever .'],\n",
       " 102: [\"But process , heavy anxiety around released took genie , genius causing nothing trouble , released back came , realized n't internalized , tormented thing .\"],\n",
       " 103: ['It could peculiar , wondrous , bizarre collaboration , kind conversation Tom strange , external thing quite Tom .'],\n",
       " 104: ['14:14 When I heard story , started shift little bit way I worked , idea already saved .'],\n",
       " 105: [\"It saved I middle writing `` Eat , Pray , Love , '' I fell one sort pit despair fall 're working something 's coming start think going disaster , worst book ever written .Not bad , worst book ever written .\"],\n",
       " 106: ['And I started think I dump project .'],\n",
       " 107: ['But I remembered Tom talking open air I tried .So I lifted face manuscript I directed comment empty corner room .'],\n",
       " 108: [\"And I said aloud , `` Listen , thing , I know book n't brilliant entirely fault , right ?\"],\n",
       " 109: [\"Because see I putting everything I , I n't .\"],\n",
       " 110: [\"If want better , 've got show part deal .\"],\n",
       " 111: [\"But n't , know , hell .\"],\n",
       " 112: [\"I 'm going keep writing anyway 's job .And I would please like record reflect today I showed part job . ''\"],\n",
       " 113: [\"15:17 ( Laughter ) 15:20 Because -- 15:22 ( Applause ) 15:24 Because end 's like , OK -- century ago desert North Africa , people used gather moonlight dance sacred dance music would go hour hour , dawn .\"],\n",
       " 114: ['They always magnificent , dancer professional terrific , right ?'],\n",
       " 115: ['But every , rarely , something would happen , one performer would actually become transcendent .'],\n",
       " 116: [\"And I know know I 'm talking , I know 've seen , point life , performance like .\"],\n",
       " 117: [\"It like time would stop , dancer would sort step kind portal n't anything different ever done , 1,000 night , everything would align .\"],\n",
       " 118: ['And sudden , would longer appear merely human .'],\n",
       " 119: ['He would lit within , lit lit fire divinity .16:14 And happened , back , people knew , know , called name .'],\n",
       " 120: [\"They would put hand together would start chant , `` Allah , Allah , Allah , God , God , God . ''That 's God , know .\"],\n",
       " 121: [\"Curious historical footnote : Moors invaded southern Spain , took custom pronunciation changed century `` Allah , Allah , Allah , '' `` Olé , olé , olé , '' still hear bullfight flamenco dance .\"],\n",
       " 122: [\"In Spain , performer done something impossible magic , `` Allah , olé , olé , Allah , magnificent , bravo , '' incomprehensible , -- glimpse God .\"],\n",
       " 123: ['Which great , need .'],\n",
       " 124: [\"16:58 But , tricky bit come next morning , dancer , wake discovers 's Tuesday 11 a.m. , 's longer glimpse God .\"],\n",
       " 125: [\"He 's aging mortal really bad knee , maybe 's never going ascend height .\"],\n",
       " 126: [\"And maybe nobody ever chant God 's name spin , rest life ?\"],\n",
       " 127: ['This hard .'],\n",
       " 128: [\"This one painful reconciliation make creative life .But maybe n't quite full anguish never happened believe , first place , extraordinary aspect came .But maybe believed loan unimaginable source exquisite portion life passed along 're finished , somebody else .And , know , think way , start change everything .\"],\n",
       " 129: [\"17:55 This I 've started think , certainly I 've thinking last month I 've working book soon published , dangerously , frighteningly over-anticipated follow freakish success .\"],\n",
       " 130: [\"18:09 And I sort keep telling I get really psyched n't afraid .\"],\n",
       " 131: [\"Do n't daunted .\"],\n",
       " 132: ['Just job .'],\n",
       " 133: ['Continue show piece , whatever might .'],\n",
       " 134: ['If job dance , dance .'],\n",
       " 135: [\"If divine , cockeyed genius assigned case decides let sort wonderment glimpsed , one moment effort , `` Olé ! ''\"],\n",
       " 136: ['And , dance anyhow .'],\n",
       " 137: [\"And `` Olé ! '', nonetheless .I believe I feel must teach .`` Olé ! ''\"],\n",
       " 138: [', nonetheless , sheer human love stubbornness keep showing .'],\n",
       " 139: ['18:51 Thank .18:53 ( Applause ) 18:55 Thank .'],\n",
       " 140: ['18:56 ( Applause ) 18:59 June Cohen : Olé !'],\n",
       " 141: ['19:01 ( Applause )']}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine it!\n",
    "data_combined = {key: [combine_text(value)] for (key, value) in data.items()}\n",
    "data_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9e9dd9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:00 I writer .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Writing book profession 's , course .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It also great lifelong love fascination .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And I n't expect 's ever going change .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But , said , something kind peculiar happened recently life career , caused recalibrate whole relationship work .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>And `` Olé ! '', nonetheless .I believe I feel must teach .`` Olé ! ''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>, nonetheless , sheer human love stubbornness keep showing .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>18:51 Thank .18:53 ( Applause ) 18:55 Thank .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>18:56 ( Applause ) 18:59 June Cohen : Olé !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>19:01 ( Applause )</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                            transcript\n",
       "0                                                                                                     00:00 I writer .\n",
       "1                                                                                Writing book profession 's , course .\n",
       "2                                                                            It also great lifelong love fascination .\n",
       "3                                                                              And I n't expect 's ever going change .\n",
       "4    But , said , something kind peculiar happened recently life career , caused recalibrate whole relationship work .\n",
       "..                                                                                                                 ...\n",
       "137                                             And `` Olé ! '', nonetheless .I believe I feel must teach .`` Olé ! ''\n",
       "138                                                       , nonetheless , sheer human love stubbornness keep showing .\n",
       "139                                                                      18:51 Thank .18:53 ( Applause ) 18:55 Thank .\n",
       "140                                                                        18:56 ( Applause ) 18:59 June Cohen : Olé !\n",
       "141                                                                                                 19:01 ( Applause )\n",
       "\n",
       "[142 rows x 1 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can either keep it in dictionary format or put it into a pandas dataframe\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',150)\n",
    "\n",
    "combined_sent = pd.DataFrame.from_dict(data_combined).transpose()\n",
    "combined_sent.columns = ['transcript']\n",
    "combined_sent = combined_sent.sort_index()\n",
    "combined_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6bcaa867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle it for later use\n",
    "combined_sent.to_pickle(\"Combined_wrt_topics.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a035975a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'00:00': 0,\n",
       " '01:07': 9,\n",
       " '01:39': 13,\n",
       " '01:42': 13,\n",
       " '02:53': 24,\n",
       " '03:53': 30,\n",
       " '04:17': 35,\n",
       " '05:11': 42,\n",
       " '05:14': 43,\n",
       " '05:58': 47,\n",
       " '06:35': 53,\n",
       " '07:05': 56,\n",
       " '07:34': 61,\n",
       " '08:10': 64,\n",
       " '08:47': 68,\n",
       " '09:30': 74,\n",
       " '09:59': 78,\n",
       " '11:31': 88,\n",
       " '11:40': 88,\n",
       " '12:17': 94,\n",
       " '12:41': 95,\n",
       " '13:05': 97,\n",
       " '13:26': 101,\n",
       " '13:44': 104,\n",
       " '14:14': 107,\n",
       " '15:17': 118,\n",
       " '16:14': 124,\n",
       " '16:58': 129,\n",
       " '17:55': 136,\n",
       " '18:09': 136,\n",
       " '18:51': 147,\n",
       " '18:53': 147,\n",
       " '18:56': 147,\n",
       " '19:01': 147}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_and_sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
